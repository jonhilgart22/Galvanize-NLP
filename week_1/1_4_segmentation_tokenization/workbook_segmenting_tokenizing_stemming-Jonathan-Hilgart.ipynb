{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmenting, Tokenizing, & Stemming\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By The End of This Workbook You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Tokenize words \n",
    "- List the advantages and disadvantages of stemming\n",
    "- Select between different stemming algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----\n",
    "Tokenization\n",
    "-----\n",
    "\n",
    "Toenization: Breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens\n",
    "\n",
    "Not dressing up like _Lord of the Rings_\n",
    "![](http://www.coolest-homemade-costumes.com/images/great-costume-idea-01.jpg)\n",
    "\n",
    "That is Tolkenization ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to tokenize is to split on white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sky', 'is', 'blue', 'and', 'trees', 'are', 'green']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = 'Sky is blue and trees are green'\n",
    "sentence1.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you might also want to deal with abbreviations, hypenations, puntuations and other characters.\n",
    "\n",
    "In those cases, you would want to use regex.\n",
    "\n",
    "However, going through a sentence multiple times can be slow to run if the corpus is long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'state', 'of', 'the', 'art', 'technology', 'is', 'cool', 'is', 'not', 'it']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence2 = 'This state-of-the-art technology is cool, isn\\'t it?'\n",
    "sentence2 = re.sub('-', ' ', sentence2)\n",
    "sentence2 = re.sub('[,|.|?]', '', sentence2)\n",
    "sentence2 = re.sub('n\\'t', ' not', sentence2)\n",
    "sentence2_tokens = re.split('\\s+', sentence2)\n",
    "print(sentence2_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there are 11 tokens and the size of the vocabulary is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 11\n",
      "Number of vocabulary: 10\n"
     ]
    }
   ],
   "source": [
    "print('Number of tokens:', len(sentence2_tokens))\n",
    "print('Number of vocabulary:', len(set(sentence2_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "TODO: Tokenize the follow sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence3 = 'If only Bradley\\'s arm was longer. Best photo ever. #oscars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'only',\n",
       " 'Bradleys',\n",
       " 'arm',\n",
       " 'was',\n",
       " 'longer',\n",
       " 'Best',\n",
       " 'photo',\n",
       " 'ever',\n",
       " 'oscars']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence3 = re.sub('[\\'.#]+','',sentence3)\n",
    "sentence3 = sentence3.split(\" \")\n",
    "sentence3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tokens\n",
      "10 vocab\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence3),'tokens')\n",
    "print(len(set(sentence3)),'vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Count the number of tokens and vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>\n",
    "Click here for a solution.\n",
    "</summary>\n",
    "`\n",
    "sentence3 = re.sub('[#|\\'|.]', '' , sentence3).lower()\n",
    "print re.split('\\s+', sentence3)\n",
    "\n",
    "10 tokens and 10 vocabulary\n",
    "`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----\n",
    "Stemming\n",
    "----\n",
    "\n",
    "Stemming removes affixes to reduce to tokens to their base form (stems)\n",
    "\n",
    "**For example:**\n",
    "\n",
    "`automates`, `automating` and `automatic` could be stemmed to `automat`\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of commonly used stemmers, and each consists of slightly different rules for systematically replacing affixes in tokens. In general, Lancaster stemmer stems the most aggresively, i.e. removing the most suffix from the tokens, followed by Snowball and Porter \n",
    "\n",
    "1. **Porter Stemmer:**\n",
    "\n",
    "    - Most commonly used stemmer and the most gentle stemmers\n",
    "    - The most computationally intensive of the algorithms (Though not by a very significant margin)\n",
    "    - The oldest stemming algorithm in existence\n",
    "\n",
    "2. **Snowball Stemmer:**\n",
    "\n",
    "    - Universally regarded as an improvement over the Porter Stemmer\n",
    "    - Slightly faster computation time than the Porter Stemmer\n",
    "\n",
    "3. **Lancaster Stemmer:**\n",
    "    \n",
    "    - Very aggressive stemming algorithm\n",
    "    - With Porter and Snowball Stemmers, the stemmed representations are usually fairly intuitive to a reader\n",
    "    - With Lancaster Stemmer, shorter tokens that are stemmed will become totally obfuscated\n",
    "    - The fastest algorithm and will reduce the vocabulary\n",
    "    - However, if one desires more distinction between tokens, Lancaster Stemmer is not recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "The following code demonstartes the difference between the different stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['player', 'playa', 'playa', 'pleyaz']\n",
      "Snowball Stemmer: ['player', 'playa', 'playa', 'pleyaz']\n",
      "Lancaster Stemmer: ['play', 'play', 'playa', 'pleyaz']\n"
     ]
    }
   ],
   "source": [
    "from nltk import stem\n",
    "\n",
    "tokens =  ['player', 'playa', 'playas', 'pleyaz'] \n",
    "\n",
    "# Define Porter Stemmer\n",
    "porter = stem.porter.PorterStemmer()\n",
    "# Define Snowball Stemmer\n",
    "snowball = stem.snowball.EnglishStemmer()\n",
    "# Define Lancaster Stemmer\n",
    "lancaster = stem.lancaster.LancasterStemmer()\n",
    "\n",
    "print('Porter Stemmer:', [porter.stem(i) for i in tokens])\n",
    "print('Snowball Stemmer:', [snowball.stem(i) for i in tokens])\n",
    "print('Lancaster Stemmer:', [lancaster.stem(i) for i in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Why would one use a stemmer during tokenization? Think of the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to use a stemmer so that you can capture duplicate words that have the same meaning. For example, being and be have the same stem and are not different vocabulary wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a fast stemmer, which stemmer would you choose? What is the disadvatange of that stemmer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancaster Stemmer is the fastest.  Disadvantages are that it obfuscates the stems so they are not readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise if you want a stemmer that preserves the original word as much as possible and still have a reasonable speed, which stemmer would you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>\n",
    "Click here for a solution.\n",
    "</summary>\n",
    "\n",
    "```\n",
    "- To reduce the size of the vocabulary. For example, we can consider \"think\" and \"thinks\" to be same token since they carry the same meaning almost all the time\n",
    "\n",
    "- Lancaster is the fastest stemmer but it would obfuscate smaller words, making it impossible for humans to tell what word it was originally\n",
    "\n",
    "- Snowball Stemmer is an improvement of the Porter Stemmer and it strikes a balance between quality and speed\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Tokenization separates words in a sentence\n",
    "- You would normalize or process the sentence during tokenization to obtain sensible tokens\n",
    "- These normalizations include:\n",
    "  - Replaceing special characters with spaces such as `,.-=!` using regex\n",
    "  - Lowercasing\n",
    "  - Stemming to remove the suffix of tokens to make tokens more uniform\n",
    "- There are three types of commonly used stemmers. They are Porter, Snowball and Lancaster\n",
    "- Lancaster is the fastest and also the most aggressive stemmer and Snowball is a good balance between speed and quality of stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
