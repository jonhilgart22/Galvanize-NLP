{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Exercises: Topic Modeling with Non-Negative Matrix factorization (NMF) \n",
    "----\n",
    "\n",
    "Today we will apply the Non-Negative Matrix factorization (NMF) algorithm to discover latent topics in New York Times articles.\n",
    "\n",
    "![](http://1.bp.blogspot.com/_JNTikHKvtnY/S6tLPRWmxjI/AAAAAAAABcQ/-eszxl-WIQ0/s1600/New-York-Times.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Preprocessing\n",
    "----\n",
    "\n",
    "1) Read the articles.pkl file using the `df.read_pickle` function in Pandas and transform the data into a structure for sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../corpora/nyt_articles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_type</th>\n",
       "      <th>web_url</th>\n",
       "      <th>lead_paragraph</th>\n",
       "      <th>abstract</th>\n",
       "      <th>snippet</th>\n",
       "      <th>news_desk</th>\n",
       "      <th>word_count</th>\n",
       "      <th>source</th>\n",
       "      <th>section_name</th>\n",
       "      <th>subsection_name</th>\n",
       "      <th>_id</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>print_page</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>article</td>\n",
       "      <td>http://www.nytimes.com/2013/10/03/sports/footb...</td>\n",
       "      <td>You would think that in a symmetric zero-sum s...</td>\n",
       "      <td>None</td>\n",
       "      <td>You would think that in a symmetric zero-sum s...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>347</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>524d4e3a38f0d8198974001f</td>\n",
       "      <td>2013-10-03T00:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Week 5 Probabilities: Why Offense Is More Impo...</td>\n",
       "      <td>the original goal building model football fore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>article</td>\n",
       "      <td>http://www.nytimes.com/2013/10/03/us/new-immig...</td>\n",
       "      <td>House Democrats on Wednesday unveiled an immig...</td>\n",
       "      <td>House Democrats unveil immigration bill that p...</td>\n",
       "      <td>House Democrats on Wednesday unveiled an immig...</td>\n",
       "      <td>National</td>\n",
       "      <td>83</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>None</td>\n",
       "      <td>524cf71338f0d8198973ff7b</td>\n",
       "      <td>2013-10-03T00:00:00Z</td>\n",
       "      <td>21</td>\n",
       "      <td>New Immigration Bill Put Forward</td>\n",
       "      <td>house unveiled immigration bill provides path ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_type                                            web_url  \\\n",
       "0       article  http://www.nytimes.com/2013/10/03/sports/footb...   \n",
       "1       article  http://www.nytimes.com/2013/10/03/us/new-immig...   \n",
       "\n",
       "                                      lead_paragraph  \\\n",
       "0  You would think that in a symmetric zero-sum s...   \n",
       "1  House Democrats on Wednesday unveiled an immig...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0                                               None   \n",
       "1  House Democrats unveil immigration bill that p...   \n",
       "\n",
       "                                             snippet news_desk word_count  \\\n",
       "0  You would think that in a symmetric zero-sum s...    Sports        347   \n",
       "1  House Democrats on Wednesday unveiled an immig...  National         83   \n",
       "\n",
       "               source section_name subsection_name                       _id  \\\n",
       "0  The New York Times       Sports    Pro Football  524d4e3a38f0d8198974001f   \n",
       "1  The New York Times         U.S.            None  524cf71338f0d8198973ff7b   \n",
       "\n",
       "               pub_date print_page  \\\n",
       "0  2013-10-03T00:00:00Z       None   \n",
       "1  2013-10-03T00:00:00Z         21   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Week 5 Probabilities: Why Offense Is More Impo...   \n",
       "1                   New Immigration Bill Put Forward   \n",
       "\n",
       "                                             content  \n",
       "0  the original goal building model football fore...  \n",
       "1  house unveiled immigration bill provides path ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look that data...\n",
    "# What are the columns?\n",
    "# What are the rows?\n",
    "df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       the original goal building model football fore...\n",
       "1       house unveiled immigration bill provides path ...\n",
       "2       federal judge wednesday ordered appointment in...\n",
       "3       texas nationâ€™s active death-penalty state turn...\n",
       "4       rafael nadal aiming end novak djokovicâ€™s run 1...\n",
       "5       brazilian labor judge ordered suspension const...\n",
       "6       defending champion bayern munich produced powe...\n",
       "7       simone bile used superb floor vault exercise s...\n",
       "8       the devil announced cory schneider would start...\n",
       "9       lindsey vonnâ€™s surgically repaired right knee ...\n",
       "10      the whitney handicap winner cross traffic run ...\n",
       "11      atlanta at least eight people died front tire ...\n",
       "12      farmer waste management company energy industr...\n",
       "13      juba south sudan even standard fashion model w...\n",
       "14      to editor read interest â€œthree sister not chek...\n",
       "15      to editor re â€œdata back bloomberg disparity wi...\n",
       "16      safety big issue american many danger persist ...\n",
       "17      a government shutdown dragged gloom mounted ja...\n",
       "18      a national zoo close congressional zoo remains...\n",
       "19      glasgow scotland in year scotland hold referen...\n",
       "20      president hassan rouhani iran wednesday dismis...\n",
       "21      glendale ariz. just three month ago phoenix co...\n",
       "22      an armed mob broke russian embassy compound li...\n",
       "23      new satellite imagery offer sign north korea r...\n",
       "24      the international criminal court issued arrest...\n",
       "25      court paris wednesday threw lawsuit claimed il...\n",
       "26      in mid-1990s baseball added division series pl...\n",
       "27      california jury decided wednesday michael jack...\n",
       "28      washington debate federal government shutdown ...\n",
       "29      pedigree matter maybe pet spouse television cr...\n",
       "                              ...                        \n",
       "1375    united nation prime minister benjamin netanyah...\n",
       "1376    by tuesday morning leadership failure speaker ...\n",
       "1377    london he tall mean provocative big-game winne...\n",
       "1378    twenty month ago eli manning giant won super b...\n",
       "1379    million american visited new online health ins...\n",
       "1380    minneapolis osmo vanska resigned tuesday music...\n",
       "1381    washington if symbol tuesday americaâ€™s pent-up...\n",
       "1382    beirut lebanon an advance team organization ch...\n",
       "1383    global investor reacted calmly tuesday governm...\n",
       "1384    onibury england near englandâ€™s border wale sch...\n",
       "1385    arlington tex the first person david price loo...\n",
       "1386    when bmw i3 finally roll year europe next year...\n",
       "1387    to editor re â€œhealth insurance exchange scramb...\n",
       "1388    khartoum sudan the worst week violent unrest c...\n",
       "1389    among 12- 17-year-olds 47 percent girl 34 perc...\n",
       "1390    pittsburgh it natural cincinnati red first pla...\n",
       "1391    the federal reserve confounded bond market las...\n",
       "1392    rome for outside italy assumed era silvio berl...\n",
       "1393    greenburgh n.y. it didnâ€™t take long steve mill...\n",
       "1394    washington two senior marine corp general orde...\n",
       "1395    following president obamaâ€™s remark white house...\n",
       "1396    richmond va. in music business still talk â€œlar...\n",
       "1397    london jobless briton could forced community w...\n",
       "1398    kabul afghanistan so many people buried alive ...\n",
       "1399    washington neither president obama prime minis...\n",
       "1400    washington flurry last-minute move house senat...\n",
       "1401    how carnage upscale shopping mall nairobi link...\n",
       "1402    baghdad several explosion suicide bomber struc...\n",
       "1403    minneapolis it past quitting time new textile ...\n",
       "1404    one sunny friday morning late january wendy da...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at a sample of the data - content of the news stories\n",
    "df.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Use the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit-learn to turn the content of the news stories into the document-term matrix $\\textbf{A}$ \n",
    "\n",
    "(I call it \"vectorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here are some good defaults\n",
    "\n",
    "\n",
    "max_features=1000\n",
    "max_df=0.95,  \n",
    "min_df=2,\n",
    "max_features=1000,\n",
    "stop_words='english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorized = CountVectorizer(max_features=1000,max_df=.95,min_df=2,stop_words='english')\n",
    "doc_term = vectorized.fit_transform(df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1405,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(df.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of the document-term matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1405x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 135271 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1405 docs by 1000 terms is the matrix size\n",
    "doc_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "NMF with scikit-learn \n",
    "------\n",
    "\n",
    "Hint: [Here is an example](http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "assert sklearn.__version__ == '0.18' # Make sure we are in the modern age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply NMF with SVD-based initialization to the document-term matrix $\\text{A}$ generate 4 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NMF(init=\"nndsvd\",\n",
    "            n_components=4,\n",
    "            max_iter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the factors $\\text{W}$ and $\\text{H}$ from the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = model.fit_transform(doc_term)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   0.00000000e+00,   6.00670745e-02,\n",
       "         7.19590348e-02,   0.00000000e+00,   2.96473692e-02,\n",
       "         2.54871183e-02,   1.09730065e-02,   0.00000000e+00,\n",
       "         7.08346396e-02,   0.00000000e+00,   4.05933677e-01,\n",
       "         7.82687398e-01,   7.21490987e-01,   3.73559476e-01,\n",
       "         1.70544914e-01,   2.99002649e-01,   1.66389567e-01,\n",
       "         0.00000000e+00,   4.08411481e-01,   0.00000000e+00,\n",
       "         5.54626515e-01,   1.86451654e-01,   1.40120017e-01,\n",
       "         0.00000000e+00,   1.12657534e-01,   0.00000000e+00,\n",
       "         7.58065691e-01,   9.12978787e-02,   1.18726844e-01,\n",
       "         8.59929137e-02,   0.00000000e+00,   3.97143189e-02,\n",
       "         9.54671582e-02,   2.67524960e-01,   2.09066979e-01,\n",
       "         1.36032642e+00,   8.52540223e-01,   1.86869149e-01,\n",
       "         4.05289948e-02,   7.41148653e-01,   7.54246731e-01,\n",
       "         1.11175020e+00,   1.25670998e-01,   3.12460244e-01,\n",
       "         3.79526966e-01,   6.11516237e-01,   8.66073649e-02,\n",
       "         0.00000000e+00,   1.53074830e-01,   4.36501355e-01,\n",
       "         1.18241403e+00,   6.92827580e-02,   2.81402591e-02,\n",
       "         1.71615437e-02,   8.65727547e-05,   1.58086552e-02,\n",
       "         0.00000000e+00,   0.00000000e+00,   1.08456304e-01,\n",
       "         1.08653523e-01,   5.66939783e-01,   3.51889973e-01,\n",
       "         7.05248190e-01,   2.10208323e-02,   0.00000000e+00,\n",
       "         2.93445029e-02,   6.73799054e-02,   2.82335358e-01,\n",
       "         3.18172794e-01,   6.10941211e-01,   3.40261750e-01,\n",
       "         1.02640463e+00,   0.00000000e+00,   4.46881086e-01,\n",
       "         4.47441626e-02,   8.13507008e-02,   2.24728968e-01,\n",
       "         2.27420510e-01,   1.32539664e-01,   2.55763797e-01,\n",
       "         8.13133912e-01,   2.06083386e-01,   2.57817358e-01,\n",
       "         2.11788733e-01,   1.54937617e-01,   8.91375950e-01,\n",
       "         8.55417416e-02,   2.03884931e-02,   2.89511397e-01,\n",
       "         4.78790176e-02,   4.38480545e-01,   8.83200503e-02,\n",
       "         7.58725377e-01,   9.05630581e-02,   7.50528377e-02,\n",
       "         2.99228350e-01,   7.27082623e-02,   5.93518087e-01,\n",
       "         4.24370014e-01])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[:100,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.97109492,  1.08278721,  0.50624774, ...,  1.93792322,\n",
       "         0.80479987,  0.32067983],\n",
       "       [ 0.14001152,  1.14926156,  0.15534813, ...,  0.15976554,\n",
       "         0.48232903,  0.04306665],\n",
       "       [ 0.34525608,  0.10497164,  0.05540141, ...,  0.04922689,\n",
       "         0.        ,  0.07341733],\n",
       "       [ 0.05793673,  0.10327576,  0.03387658, ...,  0.71556379,\n",
       "         0.2551675 ,  0.        ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is are sizes of W and H?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1405, 4)\n",
      "(4, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(W)) #docs by topics\n",
    "print(np.shape(H)) # topics by terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of all terms whose indices correspond to the columns of the document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = [\"\"] * len(vectorized.vocabulary_)\n",
    "for term in vectorized.vocabulary_.keys():\n",
    "    terms[vectorized.vocabulary_[term]] = term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Have a look that some of the terms\n",
    "terms=np.array(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  1  Top 10 terms : ['said', 'year', 'new', 'people', 'state', 'company', 'gun', 'work', 'like', 'percent']\n",
      "Topic:  2  Top 10 terms : ['game', 'season', 'said', 'team', 'year', 'player', 'time', 'play', 'yankee', 'league']\n",
      "Topic:  3  Top 10 terms : ['republican', 'government', 'house', 'health', 'law', 'care', 'party', 'shutdown', 'senate', 'president']\n",
      "Topic:  4  Top 10 terms : ['mr', 'said', 'iran', 'rouhani', 'united', 'nuclear', 'president', 'obama', 'state', 'netanyahu']\n"
     ]
    }
   ],
   "source": [
    "for c,topic in enumerate(H):\n",
    "    count = 0\n",
    "    current_terms = []\n",
    "    for term in  terms[np.argsort(topic)[::-1]]:\n",
    "        #print(term)\n",
    "        current_terms.append(term)\n",
    "        count+=1\n",
    "        if count ==10:\n",
    "            break\n",
    "           \n",
    "    print('Topic: ',c+1,' Top 10 terms :',current_terms)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top 10 ranked terms for each topic, by sorting the values in the rows of the $\\text{H}$ factor \n",
    "\n",
    "<br>\n",
    "\n",
    "<details><summary>\n",
    "Click here for a hintâ€¦\n",
    "</summary>\n",
    "```\n",
    "for topic_index in None:\n",
    "    top_indices = np.argsort(None)[None][None]\n",
    "    term_ranking = [None[i] for i in None]\n",
    "    print(\"Topic {}: {}\".format(topic_index, \", \".join(term_ranking)))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details><summary>\n",
    "Click here for the answerâ€¦\n",
    "</summary>\n",
    "```\n",
    "for topic_index in range(H.shape[0]):\n",
    "    top_indices = np.argsort(H[topic_index,:])[::-1][0:10]\n",
    "    term_ranking = [terms[i] for i in top_indices]\n",
    "    print(\"Topic {}: {}\".format(topic_index, \", \".join(term_ranking)))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Topic 1: State Gun Regulation in the New Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Topic 2: Yankee lead the baseball season\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Topic 3: The Republican Government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Topic 4:Iran Nuclear Deal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the words in the numbered topics. For each one, make-up a label that describes it.\n",
    "\n",
    "For example:  \n",
    "`Topic 3: people, mobile, said, phone, technology, music, digital, users, microsoft, software`  \n",
    "is about \"The Singularity\" ðŸ˜‰\n",
    "\n",
    "Are there any topics that don't make sense (i.e., the words don't go together)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No, all of the topics seem to go together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the number of topics to match the number of topics in NYT section labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sports', 'U.S.', 'Business Day', 'World', 'Opinion', 'Arts',\n",
       "       'Travel', 'Magazine', 'Real Estate', 'Books'], dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.section_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_2 = NMF(init=\"nndsvd\",\n",
    "            n_components=10,\n",
    "            max_iter=200)\n",
    "\n",
    "W_2 = model_2.fit_transform(doc_term)\n",
    "H_2 = model_2.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  1  Top 10 terms : ['said', 'year', 'day', 'people', 'official', 'case', 'time', 'decision', 'court', 'added']\n",
      "Topic:  2  Top 10 terms : ['game', 'season', 'team', 'year', 'player', 'time', 'league', 'yankee', 'run', 'play']\n",
      "Topic:  3  Top 10 terms : ['republican', 'house', 'government', 'health', 'law', 'care', 'party', 'president', 'shutdown', 'obama']\n",
      "Topic:  4  Top 10 terms : ['mr', 'year', 'party', 'political', 'case', 'like', 'leader', 'state', 'court', 'night']\n",
      "Topic:  5  Top 10 terms : ['new', 'work', 'company', 'like', 'york', 'people', 'ms', 'job', 'worker', 'executive']\n",
      "Topic:  6  Top 10 terms : ['gun', 'child', 'death', 'year', 'law', 'state', 'time', 'shooting', 'old', 'killed']\n",
      "Topic:  7  Top 10 terms : ['iran', 'rouhani', 'nuclear', 'obama', 'iranian', 'netanyahu', 'president', 'israel', 'united', 'mr']\n",
      "Topic:  8  Top 10 terms : ['davis', 'state', 'story', 'texas', 'woman', 'democratic', 'city', 'new', 'republican', 'candidate']\n",
      "Topic:  9  Top 10 terms : ['percent', 'year', 'government', 'market', 'company', 'million', 'month', 'country', 'bank', 'economy']\n",
      "Topic:  10  Top 10 terms : ['united', 'government', 'syria', 'state', 'weapon', 'chemical', 'security', 'attack', 'official', 'nation']\n"
     ]
    }
   ],
   "source": [
    "for c,topic in enumerate(H_2):\n",
    "    count = 0\n",
    "    current_terms = []\n",
    "    for term in  terms[np.argsort(topic)[::-1]]:\n",
    "        #print(term)\n",
    "        current_terms.append(term)\n",
    "        count+=1\n",
    "        if count ==10:\n",
    "            break\n",
    "           \n",
    "    print('Topic: ',c+1,' Top 10 terms :',current_terms)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How do the NMF topics compare to the NYT section labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#array(['Sports', 'U.S.', 'Business Day', 'World', 'Opinion', 'Arts',\n",
    "   #    'Travel', 'Magazine', 'Real Estate', 'Books'], dtype=object)\n",
    "#They seem to match but it is harder to find a topic name using NFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Which would you use to filter your news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I would use the NYT topics since they have an additional screen on the topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat with the same modeling with [`tf-idf`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1000,max_df=.95,min_df=2,stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1405x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 135271 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_3 = NMF(init=\"nndsvd\",\n",
    "            n_components=10,\n",
    "            max_iter=200)\n",
    "\n",
    "W_3 = model_3.fit_transform(tfidf_matrix)\n",
    "H_3 = model_3.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1405, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(W_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(H_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  1  Top 10 terms : ['mr', 'said', 'court', 'case', 'judge', 'state', 'justice', 'lawyer', 'prison', 'official']\n",
      "Topic:  2  Top 10 terms : ['game', 'season', 'yard', 'team', 'said', 'league', 'player', 'coach', 'play', 'touchdown']\n",
      "Topic:  3  Top 10 terms : ['republican', 'house', 'health', 'care', 'government', 'senate', 'shutdown', 'obama', 'law', 'democrat']\n",
      "Topic:  4  Top 10 terms : ['iran', 'rouhani', 'nuclear', 'iranian', 'obama', 'israel', 'united', 'mr', 'netanyahu', 'president']\n",
      "Topic:  5  Top 10 terms : ['ms', 'music', 'art', 'new', 'work', 'like', 'dance', 'york', 'museum', 'song']\n",
      "Topic:  6  Top 10 terms : ['company', 'percent', 'said', 'market', 'year', 'million', 'bank', 'china', 'price', 'state']\n",
      "Topic:  7  Top 10 terms : ['yankee', 'rivera', 'pettitte', 'inning', 'game', 'season', 'baseball', 'run', 'pitch', 'stadium']\n",
      "Topic:  8  Top 10 terms : ['attack', 'said', 'official', 'syria', 'killed', 'people', 'government', 'police', 'security', 'mall']\n",
      "Topic:  9  Top 10 terms : ['party', 'merkel', 'government', 'germany', 'european', 'election', 'political', 'europe', 'german', 'ms']\n",
      "Topic:  10  Top 10 terms : ['cup', 'team', 'race', 'said', 'club', 'player', 'year', 'america', 'won', 've']\n"
     ]
    }
   ],
   "source": [
    "for c,topic in enumerate(H_3):\n",
    "    count = 0\n",
    "    current_terms = []\n",
    "    for term in  terms[np.argsort(topic)[::-1]]:\n",
    "        #print(term)\n",
    "        current_terms.append(term)\n",
    "        count+=1\n",
    "        if count ==10:\n",
    "            break\n",
    "           \n",
    "    print('Topic: ',c+1,' Top 10 terms :',current_terms)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that change the topics?\n",
    "\n",
    "Are they \"tighter\"? Easier to describe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The topics seem to be more closely related to eachother now. The top words almost create a sentence,\n",
    "#this is due to the way TF-IDF weights the features (more frequent and rarer words have higher weight). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Challenge Exercises\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling Your Own (RYO) NMF\n",
    "-----\n",
    "\n",
    "With the document matrix (our bags of words), we can begin implementing the NMF algorithm.  \n",
    "\n",
    "1. Create a NMF class to that is initialized with a document matrix (bag of words or tf-idf) __V__.  As arguments (in addition to the document matrix) it should also take parameters __k__ (# of latent topics) and the maximum # of iterations to perform. \n",
    "  \n",
    "  First we need to initialize our weights (__W__) and features (__H__) matrices.  \n",
    "\n",
    "2. Initialize the weights matrix (W) with (positive) random values to be a __n x k__ matrix, where __n__ is the number of documents and __k__ is the number of latent topics.\n",
    "\n",
    "2.  Initialize the feature matrix (H) to be __k x m__ where __m__ is the number of words in our vocabulary (i.e. length of bag).  Our original document matrix (__V__) is a __n x m__ matrix.  __NOTICE: shape(V) = shape(W * H)__\n",
    "\n",
    "3. Now that we have initialized our matrices and defined our cost, we can begin iterating. Update your weights and features matrices accordingly.  7. Repeat this update until convergence (i.e. change in __cost(V, W*H)__ close to 0). or until our max # of iterations.\n",
    "\n",
    "4. Assume we want to use a least-squares error metric when we update the matrices __W__ and __H__. This allows us to use the numpy.linalg.lstsq solver. \n",
    "Update __H__ by calling lstsq, holding __W__ fixed and minimizing the sum of squared errors predicting the document matrix. Since these values should all be at least 0, clip all the values in __H__ after the call to lstsq.\n",
    "\n",
    "5. Use the lstsq solver to update __W__ while holding __H__ fixed. The lstsq solver assumes it is optimizing the right matrix of the multiplication (e.g. x in the equation __ax=b__). So you will need to get creative so you can use it and have the dimensions line up correctly.  Brainstorm on paper or a whiteboard how to manipulate the matrices so lstsq can get the dimensionality correct and optimize __W__. __hint: it involves transposes.__ Clip __W__ appropriately after updating it with lstsq to ensure it is at least 0.  \n",
    "`from numpy.linalg import lstsq`\n",
    "\n",
    "6. Repeat steps 4 and 5 for a fixed number of iterations.\n",
    "\n",
    "7. Return the computed weights matrix and features matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Your NMF Function\n",
    "----\n",
    "\n",
    "1. Write a function that takes __W__, __H__ and the document matrix as arguments, and returns the mean-squared error (of __document matrix - WH__).\n",
    "\n",
    "2. Using argsort on each topic in __H__, find the index values of the words most associated with that topic.  Combine these index values with the word-names you stored in the __Preliminaries__ section to print out the most common words for each topic.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code you wrote for the __Using Your NMF Function__ on the SKlearn classifier.  How close is the output to what you found writing your own NMF classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Design an API__:\n",
    "1. Put your nmf function in an nmf class.\n",
    "2. Define a function that displays the headlines/titles of the top 10 documents for each topic.\n",
    "3. Define a function that takes as input a document and displays the top 3 topics it belongs to.\n",
    "4. Define a function that ensure consistent ordering between your nmf function and the sklearn nmf class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
