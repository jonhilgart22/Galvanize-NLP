{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/book.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "By The End Of This Session You Should Be Able To:\n",
    "---\n",
    "\n",
    "- Explain why word2vec is powerful and popular\n",
    "- Describe how word2vec is a neural network\n",
    "- Identify the common architectures of word2vec\n",
    "- Apply word2vec to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Pop Quiz\n",
    "---\n",
    "\n",
    "Do computers prefer numbers or words?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Numbers__\n",
    "<br>\n",
    "<br>\n",
    "word2vec is a series of algorithms to map words (strings) to numbers (lists of floats).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "word2vec Result\n",
    "-----\n",
    "![](images/family.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "How many dimensions are data represented in? How many dimensions would we need to represent for typical word vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "They are respresented in 2 dimensions.\n",
    "\n",
    "Typically you would need word vectors would need n-1 (a baseline word can be represented as all zeros). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why is word2vec so popular?\n",
    "----\n",
    "\n",
    "1. Creates a word \"cloud\", organized by semantic meaning.\n",
    "\n",
    "2. Converts text into a numerical form that machine learning algorithms and Deep Learning Neural Nets can then use as input.\n",
    "\n",
    "3. word2vec creates dense representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "<img src=\"images/firth.png\" style=\"width: 300px;\"/>\n",
    "\n",
    ">‚ÄúYou shall know a word\n",
    ">by the company it keeps‚Äù\n",
    "\n",
    "> \\- J. R. Firth 1957\n",
    "\n",
    "__Distributional Hypothesis__: Words that occur in the same contexts tend to have similar meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example:__  \n",
    "> ... government debt problems are turning into __banking__ crises...  \n",
    "\n",
    "> ... Europe governments needs unified __banking__ regulation to replace the hodgepodge of debt regulations...\n",
    "\n",
    "The words: _government_, _regulation_, and _debt_ probably represent some aspect of _banking_ since they frequently appear near by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does word2vec model the Distributional Hypothesis?\n",
    "---\n",
    "\n",
    "word2Vec is a very simple neural network:\n",
    "![](images/w2v_neural_net.png)\n",
    "\n",
    "[Source](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Updating the weights\n",
    "----\n",
    "\n",
    "![](images/basic_neural_net.png)\n",
    "\n",
    "1. Initialize random weights.  \n",
    "2. Calculate loss function.  \n",
    "3. Update weights via back propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "What is a loss function? Give a couple of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A lost function is how you weight your errors.\n",
    "\n",
    "For example, sum of squared residuals heavily penalizes large misses. \n",
    "\n",
    "While hinge loss ignores some errors all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does this look during training?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://player.vimeo.com/video/112168934\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x10483e128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, VimeoVideo\n",
    "\n",
    "display(VimeoVideo(112168934))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/w2v_neural_net.png)\n",
    "\n",
    "The bow-tie shape is an __autoencoder__. \n",
    "\n",
    "Autoencoders compress sparse representations into dense representation. \n",
    "\n",
    "The neural network learns the mapping that best preserves the structure of the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "The 2 architectures of word2vec\n",
    "----\n",
    "\n",
    "1. ‚ÄúContinuous bag of words‚Äù: Predict a missing word in a sentence based on the surrounding context\n",
    "\n",
    "2. ‚ÄúSkip-gram‚Äù: Each current word as an input to a log-linear classifier to predict words within a certain range before and after that current word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Continuous bag of words (CBOW) architecture\n",
    "----\n",
    "\n",
    "<img src=\"images/cbow.png\" style=\"width: 400px;\"/>\n",
    "Given the context (surronding words), predict the current word.\n",
    "\n",
    "[Detailed explanation](http://alexminnaar.com/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Skip-gram architecture\n",
    "----\n",
    "\n",
    "<img src=\"images/skip-gram.png\" style=\"width: 300px;\"/>\n",
    "Given the current word, predict the context (surrounding words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Skip-gram example\n",
    "---\n",
    "\n",
    ">‚ÄúInsurgents killed in ongoing fighting‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bigrams = [\"insurgents killed\", \"killed in\", \"in ongoing\", \"ongoing fighting\"]\n",
    "\n",
    "skip_2_bigrams = [\"insurgents killed\", \"insurgents in\", \"insurgents ongoing\", \n",
    "                  \"killed in\", \"killed ongoing\", \"killed fighting\", \n",
    "                  \"in ongoing\", \"in fighting\", \n",
    "                  \"ongoing fighting\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ">‚ÄúInsurgents killed in ongoing fighting‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tri_grams = [\"insurgents killed in\", \"killed in ongoing\", \"in ongoing fighting\"]\n",
    "\n",
    "skip_2_tri_grams = [\"insurgents killed in\", \"insurgents killed ongoing\", \"insurgents killed fighting\", \"insurgents in ongoing\", \"insurgents in fighting\", \"insurgents ongoing fighting\",\n",
    "                    \"killed in ongoing\", \"killed in fighting\", \"killed ongoing fighting\", \n",
    "                    \"in ongoing fighting\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Skip-Gram architecture, deep dive\n",
    "----\n",
    "\n",
    "![](images/skip_gram_detailed.png)\n",
    "\n",
    "The target word is now at the input layer, and the context words are on the output layer.\n",
    "\n",
    "On the output layer, instead of outputing one multinomial distribution, we are outputing C multinomial distributions. Each output is computed using the same hidden to output matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Objective function:\n",
    "![](images/multinomial_distributions.png)\n",
    "\n",
    "Because the output layer panels share the same weights,\n",
    "\n",
    "Loss function:\n",
    "![](images/skip_gram_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "CBOW vs. Skip-gram\n",
    "---\n",
    "CBOW is several times faster to train than the skip-gram and has slightly better accuracy for the frequent words.  \n",
    "\n",
    "Skip-gram works well with small amount of the training data and well represents rare words.\n",
    "\n",
    "Skip-gram tends to be the most commmon architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have word vectors, what can we do?\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Math with words!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/math.jpg\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Types of Word Math\n",
    "----\n",
    "\n",
    "1. Distance\n",
    "2. Arithmetic\n",
    "3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Distance\n",
    "---\n",
    "<br>\n",
    "<img src=\"images/family.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "Words that are related will be closer than unrelated words, thus the relationships between words can encoded as distance through the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Ways to measure distance\n",
    "----\n",
    "\n",
    "<img src=\"http://i1.wp.com/dataaspirant.com/wp-content/uploads/2015/04/euclidean.png?w=600\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://i2.wp.com/dataaspirant.com/wp-content/uploads/2015/04/manhattan.png?w=600\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "---\n",
    "Check for Understanding\n",
    "----\n",
    "\n",
    "Can Manhattan Distance be extended to more than 2 dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://i2.wp.com/dataaspirant.com/wp-content/uploads/2015/04/cosine.png?resize=610%2C468\" style=\"width: 400px;\"/>\n",
    "\n",
    "[Read more here](http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cosine similarity is most often used in NLP.\n",
    "\n",
    "Because cosine similarity is automatically normalized. It is bounded between -1 and 1, similar to a correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Words closest to ‚ÄúSweden‚Äù\n",
    "----\n",
    "\n",
    "<img src=\"images/sweden_cosine_distance.png\" style=\"width: 300px;\"/>\n",
    " \n",
    "[Source](http://deeplearning4j.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![cosine_sim](https://upload.wikimedia.org/math/f/3/6/f369863aa2814d6e283f859986a1574d.png)\n",
    "\n",
    "1 meaning exactly the same  \n",
    "0 indicating orthogonality (decorrelation)  \n",
    "‚àí1 meaning exactly opposite  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f9884f38c6f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"tests pass :)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_cos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-f9884f38c6f4>\u001b[0m in \u001b[0;36mtest_cos_sim\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mv5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mv6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m54\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    \"Calculate cosine similarity between vector 1 and 2\"\n",
    "    pass # TODO: Finish function to make tests pass\n",
    "\n",
    "def test_cos_sim():\n",
    "    v1 = np.array([1, 2, 3])\n",
    "    v2 = np.array([-1, -2, -3])\n",
    "    v3 = np.array([0, 3])\n",
    "    v4 = np.array([4, 0])\n",
    "    v5 = np.array([3, 45, 7, 2])\n",
    "    v6 = np.array([2, 54, 13, 15])\n",
    "    assert cos_sim(v1, v1) == 1.0\n",
    "    assert cos_sim(v1, v2) == -1.0\n",
    "    assert cos_sim(v3, v4) == 0.0\n",
    "    assert round(cos_sim(v5, v6), 4) == round(0.97228425171235, 4)\n",
    "    return \"tests pass :)\"\n",
    "    \n",
    "print(test_cos_sim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# A solution - using norms\n",
    "def cos_sim(v1, v2):\n",
    "   \"Calculate cosine similarity between vector 1 and 2\"\n",
    "   return v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "print(test_cos_sim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# A solution - pythonic but slow\n",
    "def dot_product(v1, v2):\n",
    "    return sum(map(lambda x: x[0] * x[1], zip(v1, v2)))\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    \"Calculate cosine similarity between vector 1 and 2\"\n",
    "    v1_len = math.sqrt(dot_product(v1, v1))\n",
    "    v2_len = math.sqrt(dot_product(v2, v2))\n",
    "    return dot_product(v1, v2) / (v1_len * v2_len)\n",
    "\n",
    "print(test_cos_sim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# A solution - unpythonic but faster\n",
    "def cos_sim(v1, v2):\n",
    "    \"Calculate cosine similarity between vector a and b\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "print(test_cos_sim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Arithmetic: Word analogies\n",
    "---\n",
    "\n",
    "![](http://multithreaded.stitchfix.com/assets/images/blog/vectors.gif)\n",
    "\n",
    "[Demo](http://rare-technologies.com/word2vec-tutorial/#app)\n",
    "\n",
    "The \"Hello, world!\" of word2vec:\n",
    "> Man is to woman as king is to queen\n",
    "\n",
    "$cos(w, king) - cos(w, man) + cos(w, woman) = cos(w, queen)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use word2vec to build data products\n",
    "----\n",
    "\n",
    "<img src=\"https://assets.toptal.io/uploads/blog/image/827/toptal-blog-image-1423052243609.jpg\" style=\"width: 400px;\"/>\n",
    "\n",
    "When I worked at an employment website, I built a recommendation engine for job seekers. The job seeker would have a resume and we would suggest jobs for them. My goal was given a current job title, suggest a \"better\" job. This would increase platform engagement.\n",
    "\n",
    "What improved job would you recommend to a Babysitter?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A Nanny. \n",
    "\n",
    "A Nanny is a Babysitter as Senior Engineer is to a Engineer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plurals\n",
    "\n",
    "![](images/plurals.png)  \n",
    "\n",
    "Different paths through word2vec space encode different relationships. More on this next time with doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Verb Tense\n",
    "----\n",
    "\n",
    "<img src=\"images/verb.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Country-Captial\n",
    "----\n",
    "\n",
    "![](images/country.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3) Clustering\n",
    "----\n",
    "\n",
    "<img src=\"http://static1.squarespace.com/static/52165be2e4b046d1ac57778c/t/55f4a66de4b016fee4ec7595/1442096821668/left.gif?format=1500w\" style=\"width: 400px;\"/>\n",
    "\n",
    "[Source](http://douglasduhaime.com/blog/clustering-semantic-vectors-with-python)\n",
    "\n",
    "Use your favorite!\n",
    "\n",
    "K-means is a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Word2vec implementation\n",
    "---\n",
    "\n",
    "1. Code\n",
    "2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Code\n",
    "----\n",
    "\n",
    "1. [Google‚Äôs TensorFlow](https://www.tensorflow.org/versions/r0.8/tutorials/word2vec/index.html)\n",
    "2. [Python‚Äôs Gensim package](https://radimrehurek.com/gensim/)  \n",
    "3. [Google‚Äôs word2vec](https://code.google.com/p/word2vec/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Corpus (aka, data in NLP)\n",
    "----\n",
    "\n",
    "> \"Data is the world's best regularizer\"\n",
    "\n",
    "You need __a lot__ of data.\n",
    "\n",
    "100 billion words is good start üòâ. 100 million will work. 10 million is a good minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "\n",
    "How do we evaluate word2vec, especially if it is built on a custom corpus?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Word2Vec is an unsupervised learning algorithm. Thus there‚Äôs no good way to objectively evaluate the result. \n",
    "\n",
    "One possible method is to compare analogies performance with pretrained Google vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- word2vec: Create a dense vector representation of words that models semantic meaning based on context\n",
    "- Word2Vec is a _relatively_ simple neural net with 1 input layer, 1 hidden layer, and 1 output layer.\n",
    "- There are 2 common ways to represent context: \n",
    "    1. CBOW: given context, predict word\n",
    "    2. skip-gram: given word, predict context\n",
    "- Once trained, any vector operations can be applied to words. The most common operations are: arithmetic, distance, and clustering.\n",
    "- Sets you up for machine learning and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Speeding up skip-gram\n",
    "---\n",
    "\n",
    "Since Skip-gram is slow (look at the architecture), the smart people at The Google optimitizated within the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> When in doubt, throw a binary tree at it.\n",
    "\n",
    "This particular binary tree is call _Hierarchical Softmax_. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "___What the hell is softmax?__\n",
    "\n",
    "It is a normalized exponential.\n",
    "\n",
    "![](images/softmax_function.png)\n",
    "\n",
    "J is the current class. \n",
    "K is all classes.\n",
    "\n",
    "Generalization of the logistic function to multi-class.\n",
    "\n",
    "Used in various probabilistic multiclass classification methods:\n",
    "\n",
    "- multinomial logistic regression\n",
    "- multiclass linear discriminant analysis\n",
    "- naive Bayes classifiers \n",
    "- artificial neural networks\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Okay then ... What is Hierarchical Softmax?\n",
    "\n",
    "![](images/binary_tree.png)\n",
    "\n",
    "Uses a binary tree as a data structure to represent all words in the vocabulary. The V words must be leaf nodes of the tree. For each leaf node, there exists an unique path from the root to the node. This path is used to estimate the probability of the word represented by the leaf node.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
