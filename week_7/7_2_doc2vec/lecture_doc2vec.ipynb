{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "doc2vec\n",
    "----\n",
    "\n",
    "![](http://img5.picload.org/image/paagccr/doc2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By the end of this session, you should be able to:\n",
    "----\n",
    "\n",
    "- Describe extensions of word2vec\n",
    "    - Dependency-Based Word Embeddings\n",
    "    - Machine Translation\n",
    "- Explain how word2vec can be extended to paragraphs and documents (doc2vec)\n",
    "- Identify applications for cutting-edge algorithms of Word Mover Distance and Thought Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "word2vec: Check for understanding\n",
    "---\n",
    "\n",
    "What is goal of word2vec in Plain English?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Create a dense vector representation of words that models semantic meaning based on context.\n",
    "\n",
    "Word2vec gives you a dictionary where each definition is just a vector of floating-point numbers. \n",
    "\n",
    "Model the latent structure of language using literal string location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why are neural networks powerful in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Capable of learning complex, arbitrary, non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What math operations are most common for word2vec embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Arithmetic, distance, and clustering. But any vector operations are meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dependency-Based Word Embeddings\n",
    "---\n",
    "\n",
    "An alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in.\n",
    "\n",
    "![](images/parse.png)\n",
    "\n",
    "Extend beyond skip-gram window, \"weighted\" by syntax (not just token distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Results**\n",
    "\n",
    "![](images/dependency_results.png)\n",
    "\n",
    "Look at \"florida\" row. How are Bag-of-words (BoW) and depdency (deps) different? \n",
    "\n",
    "BOW generates counties or cities in Florida (meronyms: part of the whole).\n",
    "\n",
    "Dependency generates other states \"brothers and sisters\" (cohyponyms: words that shares hyoponyms, belong to the same hypernym)\n",
    "\n",
    "[Source](http://www.aclweb.org/anthology/P14-2050.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Machine Translation\n",
    "---\n",
    "\n",
    "![](images/machine_translation.png)\n",
    "\n",
    "Language translations are rotations and scalings of the vector space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "How are Machine Translations learned?\n",
    "----\n",
    "\n",
    "The transform matrix can be learned by bootstrapping from a small sample (manually labeled), then extend to entire language.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create a word embedding in both languages\n",
    "2. Manually specify pairs (typically, simple concrete nouns)\n",
    "3. Find the translation matrix\n",
    "4. Apply translation matrix across entire language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Doc2Vec, the most powerful extension of word2vec\n",
    "---\n",
    "\n",
    "![](http://img5.picload.org/image/paagccr/doc2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive doc2vec\n",
    "-----\n",
    "\n",
    "![](https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAATnAAAAJDY1YThjNjdhLWVlNjAtNDk5Yy1hMDMzLWI3ZDNlZDViZjllZQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Doc2vec (aka paragraph2vec or sentence embeddings) \n",
    "-----\n",
    "\n",
    "Modifies the word2vec algorithm to larger blocks of text, such as sentences, paragraphs or entire documents. \n",
    "\n",
    "![](images/overview_word.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![](images/overview_paragraph.png)\n",
    "\n",
    "Every paragraph is mapped to a unique vector, represented by a column in matrix D and every word is also mapped to a unique vector, represented by a column in matrix W . \n",
    "The paragraph vector and word vectors are averaged or concatenated to predict the next word in a context. \n",
    "\n",
    "Each additional context does not have be a fixed length (because it is vectorized and projected into the same space).\n",
    "\n",
    "Additional parameters but the updates are sparse thus still efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "doc2vec Example: Descri.beer\n",
    "---\n",
    "\n",
    "<img src=\"https://timebusinessblog.files.wordpress.com/2013/03/85632599-e1364519588629.jpg?w=360&h=240&crop=1\" style=\"width: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do to make sense of 1.6M beer reviews?\n",
    "\n",
    "![](images/beer_space.jpg)\n",
    "\n",
    "[Demo](http://descri.beer/)\n",
    "[Source](http://www.slideshare.net/BenEverson/describeer-demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"http://descri.beer/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10473c2e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"http://descri.beer/\",\n",
    "      width=800,\n",
    "      height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Concept level document similarity\n",
    "---\n",
    "\n",
    "> The Sicilian gelato was extremely rich.\n",
    "\n",
    "vs.\n",
    "\n",
    "> The Italian ice-cream was very velvety.\n",
    "\n",
    "The statements reference the __same__ idea but share __no__ words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Word Moverâ€™s Distance (WMD)\n",
    "----\n",
    "\n",
    "![](images/wmd_illustration_1.png)\n",
    "\n",
    "Represent text documents as a weighted point cloud of embedded words. \n",
    "\n",
    "The distance between two text documents A and B is the minimum cumulative distance that words from document A need to travel to match exactly the point cloud of document B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Earth moverâ€™s distance metric (EMD)\n",
    "-----\n",
    "\n",
    "Word Moverâ€™s Distance (WMD) is a special case of the [earth moverâ€™s distance metric (EMD)](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)\n",
    "\n",
    "EMD is a method to evaluate dissimilarity between two multi-dimensional distributions in some feature space where a distance measure between single features, which we call the ground distance is given. The EMD 'lifts' this distance from individual features to full distributions.\n",
    "\n",
    "[Deep dive on EMD](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/RUBNER/emd.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Word Mover Distance Example\n",
    "----\n",
    "\n",
    "![](images/WMD_worked_example.png)\n",
    "\n",
    "State-of-the-art kNN classification accuracy but slowest metric to compute.\n",
    "\n",
    "[Source: From Word Embeddings To Document Distances](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf)\n",
    "\n",
    "[Application to Data Science](http://tech.opentable.com/2015/08/11/navigating-themes-in-restaurant-reviews-with-word-movers-distance/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thought Vectors\n",
    "---\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*KYLrhDHqAAdQaJiN1G4ytA.jpeg\" style=\"width: 400px;\"/>\n",
    "\n",
    "Geoffrey Hinton's, from Google, \"Top Secret\" new algorithm.\n",
    "\n",
    "Instead of embedding words or documents in vector space, embed thoughts in vector space. Their features will represent how each thought relates to other thoughts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> When Google farts ðŸ’¨, the rest of the world ðŸ’©\n",
    "\n",
    "It hasn't been released so it is mostly speculation. Keep your eye out for it.\n",
    "\n",
    "[Thought2vec teaser](https://wtvox.com/robotics/google-is-working-on-a-new-algorithm-thought-vectors)  \n",
    "[General introduction](http://deeplearning4j.org/thoughtvectors.html)<br>\n",
    "[Skip-Thought Vectors paper](https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Summary\n",
    "---\n",
    "- Given the properties of word2vec (e.g., large corpus input, straightforward training, and text vector output), it can be applied to a variety of problems.\n",
    "- Word2vec is another perspective on Machine Translation, rotation and translation of embedded space.\n",
    "- Other semantic meanings can be captured by using dependency parsing as context.\n",
    "- Longer pieces of text can also be embedded into the same space as words (i.e., doc2vec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bonus Materials\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "WMD Optimization\n",
    "-----\n",
    "\n",
    "This is transform a NLP problem a logistical-style problem that be solved with linear programing.\n",
    "\n",
    "![](images/WMD_problem_statement.png)\n",
    "\n",
    "![](images/WMD_linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "2 architectures for doc2vec\n",
    "---\n",
    "\n",
    "1. Distributed Memory (DM)\n",
    "2. Distributed Bag of Words (DBOW)\n",
    "\n",
    "[Source: Distributed Representations of Sentences and Documents](http://cs.stanford.edu/~quocle/paragraph_vector.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributed Memory (DM)\n",
    "\n",
    "__Highlights__:\n",
    "\n",
    "- Assign and randomly initialize paragraph vector for each doc\n",
    "- Predict next word using context words and paragraph vector\n",
    "- Slide context window across doc but keep paragraph vector fixed (hence: Distrubted Memory)\n",
    "- Update weights via SGD and backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributed Bag of Words (DBOW)\n",
    "\n",
    "__Highlights__:\n",
    "\n",
    "- ONLY use paragraph vectors (no word vectors)\n",
    "- Take a window of words in a paragraph and randomly sample which ones to predict using paragraph vector\n",
    "- Simpler, more memory efficient\n",
    "\n",
    "![](images/DBOW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
