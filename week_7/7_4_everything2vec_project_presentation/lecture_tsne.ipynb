{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) \n",
    "---\n",
    "\n",
    "<img src=\"http://www.trivial.io/content/images/2015/12/tsne-8.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Explain why t-sne is the preferred algorithm for high dimensional visualization\n",
    "- Explain how t-sne works\n",
    "- List the limitations of t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is t-sne?\n",
    "----\n",
    "\n",
    "A tool to visualize high-dimensional data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Embed high-dimensional data into a space of 2-3 dimensions, which can then be visualized in a scatter plot. \n",
    "\n",
    "Specifically, it models each high-dimensional object by a 2-3 dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does t-sne do its dark magic?\n",
    "-----\n",
    "\n",
    "It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. \n",
    "\n",
    "[demo 1](https://github.com/oreillymedia/t-SNE-tutorial)  \n",
    "[demo 2](http://cs.stanford.edu/people/karpathy/tsnejs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "t-SNE steps\n",
    "----\n",
    "\n",
    "1. t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an infinitesimal probability of being picked.\n",
    "\n",
    "2. t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence between the two distributions with respect to the locations of the points in the map. \n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kullback–Leibler divergence\n",
    "----\n",
    "\n",
    "A way to compare two probability distributions:\n",
    "\n",
    "![](images/kl_form.png)\n",
    "\n",
    "- Always nonnegative\n",
    "- Zero if and only if p = q\n",
    "- Not a true distance between distributions since it is not symmetric and does not satisfy the triangle inequality. \n",
    "\n",
    "Nonetheless, it is often useful to think of KL as a “distance” between distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P: a:1/2, b:1/4, c:1/4\n",
    "Q: a:4/12, b:3/12, d:3/12, 2/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# There are 4 outcomes\n",
    "\n",
    "# 1st distribution has the following probablities for each outcome\n",
    "p = [.5, .23, .23, .04] \n",
    "\n",
    "# 2nd distribution has the following probablities for each outcome\n",
    "q = p # The same distributions\n",
    "# q = [.4, .4, .1 , .1] # A closer distrbution\n",
    "# q = [.001, .001, .001, .98] # Futher distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.907229172\n"
     ]
    }
   ],
   "source": [
    "from numpy import log2\n",
    "\n",
    "KL = 0\n",
    "\n",
    "for index, _ in enumerate(p):\n",
    "    KL += p[index] * log2(p[index]/q[index])\n",
    "    \n",
    "print(KL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "KL divergence can be a method to compress probability distributions\n",
    "------\n",
    "\n",
    "The Kullback-Leibler divergence is the penalty you'll have to pay if you try to compress data from one distribution using a scheme optimised for another.\n",
    "\n",
    "More precisely: if your data really comes from probability distribution P, but you use a compression scheme optimised for Q, the divergence D(P||Q) is the number of extra bits you'll require to store a record of each sample from P.\n",
    "\n",
    "[Source](https://www.quora.com/What-is-a-good-laymans-explanation-for-the-Kullback-Leibler-Divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "KL in continuous space\n",
    "-----\n",
    "\n",
    "![](images/kl.png)\n",
    "\n",
    "![](images/kl_density.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Really go learn Information Theory\n",
    "-----\n",
    "\n",
    "![](http://cosmicfingerprints.com/wp-content/uploads/2009/07/noise_info_entropy.jpg)\n",
    "\n",
    "[Elements of Information Theory Second Edition](http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471241954.html) is a good staring place. It describe Information Theory in terms of statistics with lots of worked examples and good exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Can you reverse t-sne (i.e., go from low dimensions to meaningful high dimensions)? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__No.__\n",
    "\n",
    "There are many possible high dimensional space that don't \"lose\" information but don't have \"meaning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Okay - I want to use t-SNE\n",
    "----\n",
    "\n",
    "[t-sne code](https://lvdmaaten.github.io/tsne/) \n",
    "\n",
    "[Implementation in python](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
    "\n",
    "disclaimer - t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
