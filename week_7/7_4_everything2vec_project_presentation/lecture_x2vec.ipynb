{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_2vec\n",
    "----\n",
    "\n",
    "![](images/all_the_things.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By the end of this session, you should be able to:\n",
    "---\n",
    "\n",
    "- List extenstions of word2vec\n",
    "- Explain how to combine word2vec with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notable Vectorizations\n",
    "-----\n",
    "\n",
    "| Name | Embedding  | \n",
    "|:-------:|:------:| \n",
    "| [Char2Vec](http://arxiv.org/abs/1508.06615) | Character |\n",
    "| [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) | Word | \n",
    "| [GloVe](http://www-nlp.stanford.edu/pubs/glove.pdf) | Word | \n",
    "| [Doc2Vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) | Sections of text |\n",
    "| [Gene2Vec](https://davidcox143.github.io/Gene2vec/) | Functional unit of heredity |\n",
    "| [Item2Vec](https://arxiv.org/abs/1603.04259) | Things to buy |\n",
    "| [Image2Vec](https://arxiv.org/abs/1507.08818) | Image |\n",
    "| [Video2Vec](https://www.dropbox.com/s/m99k5md8461xi0s/ICIP_Paper_Revised.pdf) | Video |\n",
    "\n",
    "[Source](http://datascienceassn.org/content/table-xx2vec-algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/emjois.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "emjoi2vec\n",
    "----\n",
    "<img src=\"https://s3.amazonaws.com/instagram-static/engineering-blog/emoji-hashtags/tsne_map_tight.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "[Image](images/https://s3.amazonaws.com/instagram-static/engineering-blog/emoji-hashtags/tsne_map_tight.png)  \n",
    "[Source](http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "lda2vec Overview\n",
    "----\n",
    "\n",
    "<img src=\"images/catdog_word2vec_cropped.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "word2vec review\n",
    "----\n",
    "\n",
    "1. Set up an objective function \n",
    "2. Randomly initialize vectors\n",
    "3. Do gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given the input vector (typically word) maximize the probablity of the output vector (typically contex):\n",
    "\n",
    "$$P(v_{OUT}|v_{IN})$$\n",
    "\n",
    "Convert to probability:  \n",
    "$$softmax(v_{IN} • v_{OUT})$$\n",
    "\n",
    "Probability of choosing 1 of N discrete items.   \n",
    "Mapping from vector space to a multinomial over words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "LDA review\n",
    "---\n",
    "\n",
    "<img src=\"http://salsahpc.indiana.edu/b649proj/images/proj3_LDA%20structure.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/Acs_esny-qQ/hqdefault.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "word2vec vs. LDA\n",
    "----\n",
    "\n",
    "<img src=\"images/compare_models.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "| algorithm | scope | prediction | numbers | visualization | density | metaphor| \n",
    "|:-------:|:------:| :------:| :------:| :------:| :------:|  :------:|\n",
    "| word2vec | local | one word predicts a nearby words | real numbers | bar chart | dense | location  |\n",
    "| LDA | global | documents predict global words | percentages that sum to 100%  | pie chart | sparse | mixture| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "lda2vec\n",
    "-----\n",
    "\n",
    "<img src=\"images/lda2vec.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "$v_{doc}$ is a mixture:  \n",
    "$v_{doc}$ = a $v_{topic1}$ + b $v_{topic2}$ + ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/doc_vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "lda2vec algorithm\n",
    "----\n",
    "\n",
    "![](images/defination.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "![](images/moody_lda.png)\n",
    "\n",
    "Which word2vec training architecture does Chris Moody use in lda2vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "lda2vec Executive Summary\n",
    "----\n",
    "\n",
    "![](images/punchline.png)\n",
    "\n",
    "lda2vec adds additional context, defines context as topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "----\n",
    "\n",
    "Why is context important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Segmentation\n",
    "<img src=\"http://www.omaticsoftware.com/Portals/0/EasyDNNnews/29/iStock_000019765925_Medium.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Take Home Message\n",
    "---\n",
    "\n",
    "<img src=\"images/stat sig.gif\" style=\"width: 400px;\"/>\n",
    "\n",
    "Balance the tension between model power and interpretation.\n",
    "\n",
    "Models want to be powerful (thus complex). Business people wants to understand the model.\n",
    "\n",
    "Machine readable vs. Human readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "lda2vec implementation\n",
    "----\n",
    "\n",
    "[GitHub repo](https://github.com/cemoody/lda2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Given the properties of word2vec (e.g., large discrete input, straightforward training, and vector output), it can be applied to a variety of domains. Including\n",
    "    - emjois\n",
    "    - lda\n",
    "    - `<insert your idea here>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Simplex sidebar\n",
    "----\n",
    "\n",
    "Generalized notion of a triangle.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Pink_triangle_up.svg/2000px-Pink_triangle_up.svg.png\" style=\"width: 200px;\"/>\n",
    "2-simplex is a triangle\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/83/Tetrahedron.jpg\" style=\"width: 200px;\"/>\n",
    "3-simplex is a tetrahedron\n",
    "\n",
    "__REDACTED: HARD TO VISUALIZE 4 DIMENSIONS__\n",
    "4-simplex is a 5-cell\n",
    "\n",
    "---\n",
    "\n",
    "Simplex is has coordinates are nonnegative and sum to one. \n",
    "\n",
    "The 2-simplex is the triangle in  $ℝ^3$  whose vertices are at the coordinates (1, 0, 0), (0, 1, 0), (0, 0, 1).\n",
    "\n",
    "Simplex are extremely constrained and well-behaved. That makes machine learning models easier to interpret.\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Simplex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What other statistical concept is typically constrained to sum to 1?\n",
    "</summary>\n",
    "probabilities\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bayesian prior as a instantiation of regularization\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Review Question\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What is a Plain English definition regularization?\n",
    "</summary>\n",
    "It is a tax on complexity.\n",
    "<br>\n",
    "Small is beautiful so reduce the less helpful parts of your model\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Example of using Bayesian prior\n",
    "---\n",
    "\n",
    "In Bayesian estimation, we come up with a distribution of possible parameters using Bayes' rule: \n",
    "\n",
    "$$ P(D|θ) = \\frac{P(D|θ)P(θ)}{P(D)}$$\n",
    "\n",
    "where $P(θ)$ is known as the prior. Then, to make predictions about future events, we need to integrate over this distribution of possible $θ$.\n",
    "\n",
    "Let me give an example to make this concrete. Let's say we have a coin that comes up heads with some probability $P(θ)$. We see two heads come up. Our likelihood then becomes  $P(D|θ)=θ^2, which is clearly maximized when  $θ=1$. So, our MLE is that the coin always comes up heads, and so we predict future coins will all come up heads. We see why MLE can be a bit silly: often, it overfits the data and does not generalize well, but it is good for a first estimate.\n",
    "\n",
    "Now, let us think about the Bayesian approach. Now, let's say we initially know (our prior) that our coin has a one-half chance of having  $θ=\\frac{1}{2}$  and a one-half chance of having  $θ=1$. Let's say we observe the same two heads for coin flips. Now, let us calculate:\n",
    "\n",
    "$P(θ=1/2|D)∝P(D|θ)P(D)=1/8$   \n",
    "$P(θ=1|D)∝P(D|θ)P(D)=1/2$\n",
    "\n",
    "Normalizing, we see that we have a 1/5 chance of having a fair coin, and a 4/5 chance of having a coin that always comes up heads. So, we estimate that a new coin flip would come up heads 1/5(1/2) + 4/5(1) = 9/10 of the time. Based on our prior beliefs, we would come up with different answers, but we see that in some sense, the Bayesian estimate is more \"reasonable\" than just using MLE.\n",
    "\n",
    "This is called [Maximum a posteriori estimation (MAP)](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation)\n",
    "\n",
    "[Source](https://www.quora.com/Intuitively-speaking-What-is-the-difference-between-Bayesian-Estimation-and-Maximum-Likelihood-Estimation)  \n",
    "[Technical introduction to the concept](http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf)  \n",
    "[Specific application](http://papers.nips.cc/paper/2160-on-the-dirichlet-prior-and-bayesian-regularization.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
