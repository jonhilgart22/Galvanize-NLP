{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Language Modeling\n",
    "===\n",
    "\n",
    "![](images/emoji_prediction.jpg)\n",
    "[Emoji as a new independent language](http://www.bbc.com/future/story/20151012-will-emoji-become-a-new-language)\n",
    "\n",
    "My friend works at a startup that is does predictive text for emoji conversations, especially for Apple Watch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define Language Modeling and ngrams\n",
    "- Be able to calculate ngrams\n",
    "- Be able to calculate the maximum-likelihood estimation (MLE) of words for given corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "What is Language Modeling (LM)? \n",
    "----\n",
    "\n",
    "Assign probabilities to sequences of \"words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What is next word in this sequence?\n",
    "\n",
    "`5, 4, 3, 2, _`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why LM?\n",
    "-----\n",
    "\n",
    "> Language is its own best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The language is its own best model and that sufficient data can be gathered to depict typical (or atypical) language use accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "How to apply LM\n",
    "----\n",
    "\n",
    "Language model (LM) computes P(next word|previous words).\n",
    "\n",
    "P(word|history)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "P(the|its water is so transparent that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One way is to estimate this probability is from relative frequency counts:\n",
    "\n",
    "![](images/prob_the.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Maximum-likelihood estimation (MLE) \n",
    "---\n",
    "\n",
    "![](images/mle_estimate.png)\n",
    "\n",
    "`<s>`: sentence start tag  | `</s>`: sentence stop tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Check for Understanding\n",
    "-----\n",
    "\n",
    "What is `P(<s>|</s>)`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`2/3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the `P(eggs|green)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`1/1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generative Model vs Discriminative Model sidebar\n",
    "-----------\n",
    "\n",
    "A generative model learns the joint probability distribution p(x,y).\n",
    "\n",
    "A discriminative model learns the conditional probability distribution p(y|x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The data: (x,y): (1,0), (1,0), (2,0), (2, 1)\n",
    "\n",
    "| __p(x,y)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | 1/2 | 0 |\n",
    "| x=2 | 1/4 | 1/4 |\n",
    "\n",
    "\n",
    "| __p(y pipe x)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | 1 | 0 |\n",
    "| x=2 | 1/2 | 1/2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The distribution p(y|x) is the natural distribution for classifying a given example x into a class y, which is why algorithms that model this directly are called discriminative algorithms.\n",
    "\n",
    "Generative algorithms model p(x,y), which can be transformed into p(y|x) by applying Bayes rule and then used for classification. \n",
    "\n",
    "However, the distribution p(x,y) can also be used for other purposes. For example you could use p(x,y) to generate likely (x,y) pairs.\n",
    "\n",
    "[Source](http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Levels of Language Modeling\n",
    "------\n",
    "\n",
    "1. char\n",
    "2. word \n",
    "3. ngram\n",
    "4. \"refrigerator magnets\" model\n",
    "5. phrases\n",
    "6. sentences\n",
    "7. conversation (groups of sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Refrigerator Magnets model\n",
    "-------\n",
    "\n",
    "\n",
    "![](images/magents.png)\n",
    "A language is a collection of highly likely n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "What is ngram modeling?\n",
    "----\n",
    "\n",
    "- Unigram model predicts the probability of the next word based on the probability of each word (not conditional)\n",
    "- Bigram model predicts the probability of the next word based on the previous word (conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is NOT ngram modeling?\n",
    "------\n",
    "\n",
    "N-grams models do __not__ account for syntax dependencies.\n",
    "\n",
    "It is pure statistial approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Point to Ponder\n",
    "-----\n",
    "\n",
    "A language model can generalize ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For any plausible sentence with Tuesday in it,   \n",
    "there is a similar plausible sentence with Wednesday in it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That is why we need word tagging!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Language Modeling is learning the statistically patterns in a language.\n",
    "- Then assign probabilities to sequences of \"words\".\n",
    "- We will use MLE, P(w|wi-1) = c(wi-1, wi) / c(wi-1).\n",
    "- The probabilities change be assigned at any level in the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
