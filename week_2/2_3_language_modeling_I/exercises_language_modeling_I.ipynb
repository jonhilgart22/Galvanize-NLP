{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Exercises: Language Modeling\n",
    "====\n",
    "\n",
    "![](https://www.sri.com/sites/default/files/uploads/brains_languagetrans_istockphoto.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words Model\n",
    "----\n",
    "\n",
    "We take the words in a corpus to make a __generative model__ of language. \n",
    "\n",
    "We know that language is very complicated, but we can create a simplified model of language that captures part of the complexity. \n",
    "\n",
    "In the __bag of words__ model, we ignore the order of words, just count their frequency.  \n",
    "\n",
    "Think of it this way: take all the words from the text, and throw them into a bag.  Shake the bag, and then generating a sentence consists of pulling words out of the bag one at a time.  \n",
    "\n",
    "Chances are it won't be grammatical or sensible, but it will have words in roughly the right proportions.  \n",
    "\n",
    "Let's write a function to sample an *n* word sentence from a bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../../corpora/the_kama_sutra.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def words(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    return re.findall('[a-z]+', text.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'kama', 'sutra', 'of', 'vatsyayana']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "karma = words(text)\n",
    "karma[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['engaged', 'get', 'nor', 'to', 'like']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "random.shuffle(karma)\n",
    "bag = karma\n",
    "bag[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(bag, n=10):\n",
    "    \"Sample a random n-word sentence from the model described by the bag of words.\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XXX: There can't be a deterministic unit test since it is random function it should be something like...\n",
    "sample(bag) #=> 'intercourse attached he between his with things as phut to'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type of ngram model is this? Why?\n",
    "\n",
    "1. unigram\n",
    "2. bigram\n",
    "3. trigram\n",
    "4. none of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4487),\n",
      " ('of', 2957),\n",
      " ('and', 2224),\n",
      " ('to', 1560),\n",
      " ('a', 1478),\n",
      " ('her', 1081),\n",
      " ('is', 999),\n",
      " ('in', 980),\n",
      " ('should', 812),\n",
      " ('with', 717)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "counts = Counter(karma)\n",
    "pprint(counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                  count\n",
      "there                 120\n",
      "are                   425\n",
      "common                11\n",
      "and                   2224\n",
      "neverseen             0\n",
      "words                 41\n"
     ]
    }
   ],
   "source": [
    "print('{0:20}  {1}'.format(\"word\", \"count\"))\n",
    "for word in words('there are common and neverseen words'):\n",
    "    print('{0:20}  {1}'.format(word, counts[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                  frequency\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "non-empty format string passed to object.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1aef8da11670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0:20}  {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"frequency\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'there are common and neverseen words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0:20}  {1:.2}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: non-empty format string passed to object.__format__"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate the frequency of the words\n",
    "\n",
    "print('{0:20}  {1}'.format(\"word\", \"frequency\"))\n",
    "for word in words('there are common and neverseen words'):\n",
    "    print('{0:20}  {1:.2}'.format(word, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Turn that into a function\n",
    "def calc_prob_distibution(counter):\n",
    "    \"Calculate a probability distribution based on evidence from a Counter.\"\n",
    "    pass\n",
    "\n",
    "prob_distrubtion = calc_prob_distibution(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert round(prob_distrubtion(\"the\"), 4) == 0.0764\n",
    "assert round(prob_distrubtion(\"with\"), 4) == 0.0122"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, what is the probability of a *sequence* of words?  Use the definition of a joint probability:\n",
    "\n",
    "$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_1 w_2) \\ldots  \\times \\ldots P(w_n \\mid w_1 \\ldots w_{n-1})$\n",
    "\n",
    "The *bag of words* model assumes that each word is drawn from the bag *independently* of the others.  This gives us the wrong approximation:\n",
    "    \n",
    "$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2) \\times P(w_3) \\ldots  \\times \\ldots P(w_n)$\n",
    "\n",
    "It is wrong but okay enough to move forward..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def product(iterable):\n",
    "    \"Multiply the numbers together.  (Like `sum`, but with multiplication.)\"\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "def prob_words(words):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    return product(prob_distrubtion(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases = ['the the the',\n",
    "         'the son',\n",
    "         'the son of a Brahman', \n",
    "         'this is a neverbeforeseen word']\n",
    "\n",
    "for phrase in phrases:\n",
    "    print('{0:30}  {1:.6}'.format(phrase, prob_words(words(phrase))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO: Why is `the the the` so likely? What would we have to add to our model to reduce the likelihood of nonsense phrases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Why is there zero probability for a phrase with a neverbeforseen word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Challenge Exercises\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mo' Data, Mo' Better\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move up from millions to *billions and billions* of words.  \n",
    "\n",
    "Once we have that amount of data, we can start to look at two word sequences, without them being too sparse.  \n",
    "\n",
    "We happen to have data files available in the format of `\"word \\t count\"`, and bigram data in the form of `\"word1 word2 \\t count\"`.  Let's arrange to read them in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_counts(filename, sep='\\t'):\n",
    "    \"\"\"Return a Counter initialized from key-value pairs, \n",
    "    one on each line of filename.\"\"\"\n",
    "    C = Counter()\n",
    "    for line in open(filename):\n",
    "        key, count = line.split(sep)\n",
    "        C[key] = int(count)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_unigram = load_counts('../../corpora/unigram_word_counts.txt')\n",
    "counts_bigram = load_counts('../../corpora/bigram_word_counts.txt')\n",
    "\n",
    "print(len(counts_unigram))\n",
    "print(len(counts_bigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data do we have?\n",
    "\n",
    "TODO: What is the total number of values in unigrams, aka the size of the corpus?\n",
    "\n",
    "TODO: What is the total number of values in bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_unigram.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_bigram.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Bigram Model\n",
    "-----\n",
    "\n",
    "A less-wrong approximation:\n",
    "    \n",
    "$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_2) \\ldots  \\times \\ldots P(w_n \\mid w_{n-1})$\n",
    "\n",
    "This is called the *bigram* model, and is equivalent to taking a text, cutting it up into slips of paper with two words on them, and having multiple bags, and putting each slip into a bag labelled with the first word on the slip.  Then, to generate language, we choose the first word from the original single bag of words, and chose all subsequent words from the bag with the label of the previously-chosen word.\n",
    "\n",
    "Let's start by defining the probability of a single discrete event, given evidence stored in a Counter:\n",
    "\n",
    "Recall that the less-wrong bigram model approximation to English is:\n",
    "    \n",
    "$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_2) \\ldots  \\times \\ldots P(w_n \\mid w_{n-1})$\n",
    "\n",
    "where the conditional probability of a word given the previous word is defined as:\n",
    "\n",
    "$P(w_n \\mid w_{n-1}) = P(w_{n-1}w_n) / P(w_{n-1}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_dist_unigram = calc_prob_distibution(counts_unigram)\n",
    "prob_dist_bigram = calc_prob_distibution(counts_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change function to use the bigger dictionary\n",
    "def prob_words(words):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    return product(prob_dist_unigram(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cond_prob_word(word, prev):\n",
    "    \"Conditional probability of word, given previous word.\"\n",
    "    bigram = prev + ' ' + word\n",
    "    if prob_dist_bigram(bigram) > 0 and prob_dist_unigram(prev) > 0:\n",
    "        return prob_dist_bigram(bigram) / prob_dist_unigram(prev)\n",
    "    else: # Average the back-off value and zero.\n",
    "        return prob_dist_unigram(word) / 2\n",
    "\n",
    "# TODO: Finish the prob_words_two function\n",
    "def prob_words_bigram(words, prev='<S>'):\n",
    "    \"The probability of a sequence of words, using bigram data, given prev word.\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(prob_words(words('the the the'))) #=> 6.087644042127257e-05\n",
    "print(prob_words_two(words('the the the'))) #=> 7.2594323333854714e-09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Why does the probability of this phrase go down with bigram word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(prob_words(words('of the same'))) #=> 6.087644042127257e-05\n",
    "print(prob_words(words('of the same')))  #=> 0.00012825557171799635"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Why does the probability of this phrase go way up with bigram word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
