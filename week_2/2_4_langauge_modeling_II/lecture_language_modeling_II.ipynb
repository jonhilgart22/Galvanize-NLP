{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Language Modeling: Smoothing and Markov Modeling\n",
    "=====\n",
    "\n",
    "![](https://cdn.shopify.com/s/files/1/0823/1755/products/80359_smooth_operator_tab_patch_ocp_1_1024x1024.jpeg?v=1454440050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Describe why and how we do \"smoothing\" in language modeling\n",
    "- Use Laplace smoothing\n",
    "- Explain the Markov assumption is in general\n",
    "- Able the Markov model to language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Every time I fire a linguist, the performance of the speech recognizer goes up.  \n",
    "> \\- Frederick Jelinek\n",
    "\n",
    "Linguist often think they are clever and \"understand\" language. \n",
    "\n",
    "Linguist (and you) are __not__ clever. It is often better (and cheaper) just to get lots of data and model the statistial regularities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Smoothing\n",
    "-----\n",
    "\n",
    "![](http://images.slideplayer.com/12/3426299/slides/slide_17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Why do we want to do \"smoothing\"?\n",
    "---\n",
    "\n",
    "![](https://www.wired.com/images_blogs/business/2010/08/OED.jpg)\n",
    "\n",
    "Remember - Languages are huge and sparse. Any corpus only contains a limited number of all of possible English vocabulary.\n",
    "\n",
    "Smoothing is used to better estimate the probability of English words appearing we haven't seen yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What is the MLE for the probability of sentence that contains a \"neverbeenseen\" word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://media.giphy.com/media/xT5LMXexksEREWjrq0/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Laplace Smoothing (you may remember him from his other hit \"Probability Theory\"\n",
    "-----\n",
    "\n",
    "![](http://learn-math.info/history/photos/Laplace_2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/og.png)\n",
    "\n",
    "![](images/new.png)\n",
    "\n",
    "c is count of word  \n",
    "N is total number of tokens, len(tokens)  \n",
    "V is cardinality of tokens, len(set(tokens))  \n",
    "\n",
    "Add-one (Laplace) smoothing redistributes probability mass from __frequently occurring words__ to __all possible words.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](http://images.slideplayer.com/12/3426299/slides/slide_17.jpg)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hey that works for NLP, but what about other discrete/categorical data?\n",
    "-----\n",
    "\n",
    "> In statistics, additive smoothing, also called Laplace smoothing ... is a technique used to smooth categorical data.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Additive_smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://media.giphy.com/media/xT5LMEYx5AZwu4oiac/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can't escape the chain rule. The one rule to rule them all!\n",
    "------\n",
    "\n",
    "The probability of the next word is computed using the Chain Rule\n",
    "![](images/chain_rule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Markov Assumption\n",
    "----\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/AAMarkov.jpg/220px-AAMarkov.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the Markov assumption?\n",
    "-------\n",
    "\n",
    "> The future is independent of the past given the present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Markov model\n",
    "-----\n",
    "\n",
    "The conditional probability distribution of future states of a processs depends only upon the present state, not on the sequence of events that preceded it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Assumption that a node only depends on its immediate parents, not on all predecessors in the ordering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is a memoryless property of a stochastic process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using the Markov assumption and the chain rule, we can write a 1st-order Markov chain\n",
    "\n",
    "![](images/first_order.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "NLP sidebar\n",
    "----\n",
    "\n",
    "> NLP is just time series in disguise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Math Flashback (you have been warned!)\n",
    "-----\n",
    "\n",
    "(first-order) Markov chain is characterized by an initial distribution over states and a state transition matrix \n",
    "\n",
    "![](images/markov_matrix.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2nd order Markov chain\n",
    "-----\n",
    "\n",
    "![](images/markov_2order.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "![](images/chain_rule.png)\n",
    "\n",
    "How does the Markov Assumption change the Chain Rule for word prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Look back is limited by k (0, 1, 2, ...)\n",
    "\n",
    "![](images/word_prediction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for Understanding\n",
    "------\n",
    "\n",
    "> You are uniformly charming!‚Äù cried he, with a smile of associating and now and then I bowed and they perceived a chaise and four to wish for.\n",
    "\n",
    "Random sentence generated from a Jane Austen trigram model\n",
    "\n",
    "How does that model generate this output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- Language is large and sparse. To account for unseen words, steal a little probability from popular words give it to unpopular words.\n",
    "- Laplace Smoothing does these by adding 1 to the count of all vocabulary words.\n",
    "- Markov Assumption: The future only depends on the present.\n",
    "- That limitation reduces computational complex and data demands.\n",
    "- But can be extended backward to more previous states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus\n",
    "----\n",
    "\n",
    "![](https://pbs.twimg.com/profile_images/378800000736270833/c1a4bfd9086276bf25c525b0eed29ae6.jpeg)\n",
    "\n",
    "Automatically generated #2chainz sayings based on a Markov model\n",
    "\n",
    "https://twitter.com/markov2chainz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
