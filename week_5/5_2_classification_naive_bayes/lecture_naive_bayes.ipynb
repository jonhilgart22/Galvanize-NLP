{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes (NB) for text classification\n",
    "---\n",
    "\n",
    "![](http://img.scoop.it/_VP0qLV-jYbp2cFF4H4k4jl72eJkfbmt4t8yenImKBVvK0kTmF0xjctABnaLJIm9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "By the end of this session, you should be able to:\n",
    "---\n",
    "\n",
    "- Apply Naive Bayes formula for text classification\n",
    "- Calculate probablities for words that do not appear during training\n",
    "- Evaluate a Naive Bayes classifier (or any supervised algorithm)\n",
    "- Describe machine learning in Plain English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "----\n",
    "\n",
    "What is classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Predict amongst a discrete set of groups.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Puppy or Lion__\n",
    "![](images/prediction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3 things about ML\n",
    "-----\n",
    "\n",
    "1. Feature = numeric representation of raw data\n",
    "2. Model = mathematical “summary” of features\n",
    "3. Making something that works = choose the right model and features, given data and task\n",
    "\n",
    "Source: Understanding Feature Space in Machine Learning from Alice Zheng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Feature = numeric representation of raw data\n",
    "----\n",
    "\n",
    "![](images/features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/sparse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visualizing Feature Space\n",
    "------\n",
    "\n",
    "> Crudely speaking, mathematicians fall into two categories: the __algebraists__, who find it easiest to reduce all problems to sets of numbers and variables, and the __geometers__, who understand the world through shapes. \n",
    " \n",
    ">\\- Masha Gessen, “Perfect Rigor”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/bag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/dim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model = Mathematical “summary” of feature\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is a summary?\n",
    "\n",
    "- Data: point cloud in feature space\n",
    "- Model: a geometric shape that best “fits” the point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When does bag-of-words fail?\n",
    "------\n",
    "\n",
    "![](images/fail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Improving on bag-of-words\n",
    "-----\n",
    "\n",
    "![](images/tf-idf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/flat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MACHINE LEARNING = REPRESENTATION + EVALUATION + OPTIMIZATION\n",
    "====\n",
    "\n",
    "[Source](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Representation\n",
    "-----\n",
    "\n",
    "> A classifier must be represented in some formal language that the computer can handle. \n",
    "\n",
    ">Conversely, choosing a representation for a learner is tantamount to choosing the set of classifiers that it can\n",
    "possibly learn. This set is called the hypothesis space of the learner. If a classifier is not in the hypothesis\n",
    "space, it cannot be learned. \n",
    "\n",
    "> A related question ... is how to represent the input, i.e., what features to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Evaluation\n",
    "----\n",
    "\n",
    "> An evaluation function (also called objective function or scoring function) is needed to distinguish\n",
    "good classifiers from bad ones. \n",
    "\n",
    "> The evaluation function used internally by the algorithm may differ from the external one that we want the classifier to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optimization\n",
    "----\n",
    "\n",
    "> We need a method to search among the classifiers in the language for the highest-scoring one. \n",
    "\n",
    "> The choice of optimization technique is key to the efficiency of the learner, and also helps determine the\n",
    "classifier produced if the evaluation function has more than one optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes for text classification\n",
    "----\n",
    "\n",
    "- Representation:\n",
    "    - Data is a bag-of-words (counts or tf-idf)\n",
    "    - Classifer is a hyperplane \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Evaluation is most often a confusion matrix of predicted by observed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Optimization is done by calculating class probabilities and conditional probabilities on training data and then values of these probabilities are used to classify new observations. Predict the most probable class given a test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](http://briank.im/content/images/2015/05/Bucky.jpg)\n",
    "\n",
    "In Naive Bayes (NB), features are conditionally independent given the class. This means that each feature within each class as an only parent. \n",
    "\n",
    "Despite this assumption, the efficiency of NB has witnessed its widespread development in real world applications including medical diagnosis, fraud detection, email filtering, and A/B testing for webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Steps in Bayes Classifications\n",
    "-------\n",
    "1. Get labeled data\n",
    "1. Preprocess\n",
    "1. Apply Mulitnomial Naive Bayes\n",
    "    1. Calculate document class priors\n",
    "    1. Calculate conditional probablities of each word for each class\n",
    "    1. Calculate the proportional probablities for each class of new document\n",
    "    1. Pick the winning class\n",
    "1. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does Bayes Rule apply to document classfication?\n",
    "---\n",
    "\n",
    "![](images/bayes_rule_for_docs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "What is the MLE for a word that's never been seen before (without smoothing?) \n",
    "---\n",
    "\n",
    "![](images/mle_wo_smoothing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Smoothing for Naive Bayes\n",
    "---\n",
    "\n",
    "![](images/mle_w_smoothing.png)\n",
    "\n",
    "Start with simplest smoothing: Add-1 op Laplace\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Confusion Matrix\n",
    "---\n",
    "\n",
    "![](http://i.stack.imgur.com/ysM0Z.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's classify movies as \"RomCom\" or not...\n",
    "\n",
    "\n",
    "| Actual ↓ / Predicted → | RomCom | Not |  \n",
    "| :--: |:-------:|:------:|\n",
    "| RomCom | 70 | 50 |\n",
    "| Not RomCom| 30 | 160 |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Extension beyond 2 groups\n",
    "---\n",
    "\n",
    "| Actual↓ / Predicted→ | RomCom | Drama | Comedy |\n",
    "| :--: |:-------:|:------:|\n",
    "| RomCom | 70 | 0 | 50 |\n",
    "| Drama  | 5 | 100 |  10 |\n",
    "| Comedy | 25 | 0 |  50 |\n",
    "\n",
    "\n",
    "The classifer is misclassifing movies as Comedy when they are RomCom more often than Drama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is always important to look at the confusion matrix to analyze your results as it also gives you very strong clues as to where your classifier is going wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "----\n",
    "\n",
    "How does confusion matrix scale with number of groups? For example if have 10 categories what is the size of the matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| # of Groups | # of Cells  |  \n",
    "|:-------:|:------:|\n",
    "| 2 | 4 |\n",
    "| 3 | 9 |  \n",
    "| 4 | 16 |  \n",
    "| ... | ... |  \n",
    "| 10 | 100 |\n",
    "\n",
    "k^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Where does NB NOT work?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/anti_bayes.jpg)\n",
    "\n",
    "These groups are separable but a NB classifer will be at chance.\n",
    "\n",
    "[Source](https://www.youtube.com/watch?v=feBKiAdhYkc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Summary\n",
    "---\n",
    "\n",
    "- Machine Learning helps computers learn data and models through represents.\n",
    "- The goals is effectively and efficiently learn these representations.\n",
    "- Naive Bayes is among the simplest probabilistic classifiers. \n",
    "- Performs surprisingly well in many real world applications, despite the strong assumption that all features are conditionally independent given the class. \n",
    "- \"Smoothing\" words counts allows a NB classifier to predict features it has not yet seen\n",
    "- A confusion matrix is a way to visualize our  occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
