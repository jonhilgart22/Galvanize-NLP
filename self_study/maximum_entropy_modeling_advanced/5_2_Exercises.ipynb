{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Exercises\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will extract named entities from New York Times articles and attempt to discover sentiment about them.<br>\n",
    "<br>\n",
    "Use the NY Times articles from previous labs.\n",
    "\n",
    "1) Use the `content` of the articles. Tokenize and Tag the tokens with part-of-speech (POS) tags. Use `nltk` or `TextBlob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2) Use [`nltk.chunk.ne_chunk(tagged)`](http://www.nltk.org/api/nltk.chunk.html#nltk.chunk.ne_chunk) or [polyglot](http://polyglot.readthedocs.org/en/latest/NamedEntityRecognition.html) to recognize named entities and store them in Pandas along with the articles in a new column called `entities`. <br>\n",
    "*e.g.* `(GPE New/NNP York/NNP)` should be stored as `{'entities': {'GPE': ['New York']}`. <br>\n",
    "\n",
    "<br>__Hints__: \n",
    "- Use `set` to avoid adding duplicates within each cell. While you're doing this, keep a count of document frequency of each named entity, that is, how many different articles include \"New York\", etc.\n",
    "- Installing polyglot:\n",
    "```bash\n",
    "brew install icu4c # Install the correct C complier\n",
    "CFLAGS=`-I/usr/local/opt/icu4c/include LDFLAGS=-L/usr/local/opt/icu4c/lib pip install pyicu # Install a dependency\n",
    "pip install polyglot # Finally install the package\n",
    "polyglot download embeddings2.en ner2.en # Needed extensions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Sentiment for Entities\n",
    "- Pick a few named entities with the highest document frequency. \n",
    "- Run your sentiment classifier from last week on them (or use [TextBlob’s Sentiment Classifer](http://textblob.readthedocs.org/en/dev/advanced_usage.html)).\n",
    "- Do some entities appear to have more positive or negative sentiment attached to them? (Beware, words that indicate positive or negative sentiment in movie reviews may not carry the same implication in New York Times articles.) \n",
    "- Do a spot check on the articles classified with the greatest confidence (according to `.predict_proba`). Do you agree with your model's prediction? (*i.e.* do you think it correctly identified positive and negative sentiment?) What do you think you could do to improve your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "----\n",
    "1. Do the analysis end-to-end with 1 article. Then automate.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "***\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional:\n",
    "----\n",
    "1. Try stemming both your training (IMDb) and test (NYT) set before training and testing your sentiment analysis classifier. (*N.B.* do **not** stem before tagging.)\n",
    "2. Try improving your POS tags with word shape features as described in slide 20 of [the lecture](http://spark-public.s3.amazonaws.com/nlp/slides/Information_Extraction_and_Named_Entity_Recognition_v2.pdf) as follows:\n",
    "    1. `a-z` –> `x`, `A-Z` –> `X`, `0-9` –> `d`, `,.;:”’?!$-` –> self, other –> `*`\n",
    "    2. Trim sequences of length 3+ to 3  \n",
    "    e.g. `'apples'` –> `'xxx'`, `'Apples'` –> `'Xxxx'`, `'app9LES@'` –> `'xxx9XXX*'`  \n",
    "    Hint: use `.islower()`, `.isupper()`, and `.isdigit()`.  \n",
    "    Does that improve your NER?\n",
    "3. Try using [spaCy](https://honnibal.github.io/spaCy/)'s NER instead. Compare spaCy's to NLTK's results (spaCy should much better than but also much slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
