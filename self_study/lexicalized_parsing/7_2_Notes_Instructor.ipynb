{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2 Lexicalizing Parsing\n",
    "===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agenda\n",
    "---------------------\n",
    "1. OYO\n",
    "2. Review\n",
    "3. RAT\n",
    "4. Work Session on CKY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OYO\n",
    "----\n",
    "- Overfit the lexcial parser\n",
    "- Generalize\n",
    "\n",
    "__Options__:\n",
    "- train on wall street journal\n",
    "- train on other cropae\n",
    "- add noise\n",
    "- training less, make a worse parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Announcements\n",
    "----\n",
    "- You will get examples from gU1 Capstone written proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review\n",
    "---\n",
    "\n",
    "![](images/silly_pcfg.png)\n",
    "\n",
    "[Another example](https://spacy.io/blog/parsing-english-in-python/)\n",
    "\n",
    "PCFG don't words until the end. That is silly! We should put in the words through.\n",
    "\n",
    "\n",
    "PCFG limitations:\n",
    "1. Lack of Sensitivity to Lexical Information\n",
    "2. Lack of Sensitivity to Structural Preferences\n",
    "\n",
    "The lexicon, aka the words, can be used to calculate conditional probabilities. If the sentence has a verb \"Kicked\" that is transitive verb mostly there will be a direct object, aka noun phrase. Additionally, from the language model we can know the likeklihoods of what things are \"kicked\" (e.g., a ball, a bucket, \"it\")\n",
    "\n",
    "> Sylvia kicked Juan under the table.\n",
    "\n",
    "### Lexicalization of PCFGs\n",
    "\n",
    "\n",
    "Add “headwords” to each phrasal node\n",
    "  \n",
    "expand out a sentence\n",
    "\n",
    "Lexicalized parsers can be seen as producing dependency trees.\n",
    "Each local binary tree corresponds to an attachment in the dependency graph.\n",
    "  \n",
    "P(head_word | parent_head_word, category, parent_category)\n",
    "\n",
    "P(rule | head, category, parent_category)\n",
    "\n",
    "### Steps in Charniak parser\n",
    "1. First, parse with the base grammar   \n",
    "2. For each h calculate P(h|ph, c, pc)\n",
    "3. Then do the full CKY\n",
    "\n",
    "P(h|c=ROOT)\n",
    "\n",
    "All of this very specific conditional probabilities are too sparse/raw\n",
    "\n",
    "Therefore add a series of weights λ. Weight for each group of condtional probablities. A little bit of hand tuning\n",
    "\n",
    "__PCFG & Independence__: At each node parsing is independent, don't care where we came from .\n",
    "\n",
    "You don't lexicalization. You just use smart meta-data instead of encoding all of a language.\n",
    "\n",
    "Horizontal Markovization: Merge states, repeated categories in a row\n",
    "    trade off between size of grammera and speed\n",
    "    \n",
    "Horizontal Markovization: Look at history, less or more\n",
    "\n",
    "Takes most probable parse but may not be logical. Tag with -u. Then rewrite to be logical.\n",
    "     Unary splits time 10:33 of [https://class.coursera.org/nlp/lecture/173](https://class.coursera.org/nlp/lecture/173) \n",
    "     \n",
    "Use custom tags for task and corpus\n",
    "\n",
    "__Latent variable PCFG__: model hidden states and tranistion between them as a tree structure\n",
    "    Wait for Forward-Backward Algorithm & HMM for special topics\n",
    "\n",
    "Splits like decision trees \n",
    "    Pick your favorite method for homogeneity\n",
    "    ![](images/splits.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tree import Tree\n",
    "\n",
    "# Tree.fromstring('''(S\n",
    "#   (PP (TO to)\n",
    "#     (NP (PRP him)))\n",
    "#   (NP (PRP she))\n",
    "#   (VP (VBD was)\n",
    "#     (NP (DT a) (JJ good) (NN friend)))\n",
    "# (. .))''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAT Solutions\n",
    "-----\n",
    "1. $C^2$V Since we condition on each of C possible parent categories, C possible categories for the child, and V possible parent head words, we have roughly $C^2$V different probability distributions represented by P(h|ph,c,pc).\n",
    "2. B There are almost no true independencies in a parse tree! This is why we can get more mileage out of using a lexicalized PCFG or (as we will see later) by using more specific symbols than the traditional non-terminal symbols generally used.\n",
    "3. B Nothing changes about the CKY algorithm, except the size and running time. We still have a PCFG and can run the CKY algorithm just as presented earlier.\n",
    "4. `(S-was\n",
    "    (PP-to (TO to)\n",
    "      (NP-him (PRP him)))\n",
    "    (NP-she (PRP she))\n",
    "    (VP-was (VBD was)\n",
    "      (NP-friend (DT a) (JJ good) (NN friend))))\n",
    "  (. .))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "----\n",
    "\n",
    "Scrum Questions\n",
    "-----\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. What did my pair do yesterday?\n",
    "2. What does my pair plan to do today?\n",
    "3. What are my pair's obstacles or rate limiters?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
