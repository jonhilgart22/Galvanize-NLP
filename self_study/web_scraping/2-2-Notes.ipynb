{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2-Scraping\n",
    "=====================\n",
    "Agenda\n",
    "---------------------\n",
    "2. Questions\n",
    "3. Teams\n",
    "7. Pair [Exercise](#2.2-Exercise)\n",
    "7. Presentations\n",
    "8. Peer Review\n",
    "9. [Next Time](#Before-Wednesday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 1: Using the PyMongo Client\n",
    "\n",
    "The PyMongo Client resembles Python much more than the Mongo shell we have dealt with this morning. In most applications, you will use PyMongo to interact with your Mongo Database as part of your data pipeline. \n",
    "\n",
    "<br>\n",
    "\n",
    "1. Use the follow snippet to start a Pymongo client and create a new collection (table) within a new database.   \n",
    "  \n",
    "   ```python\n",
    "   from pymongo import MongoClient\n",
    "   client = MongoClient()\n",
    "   # Initiate Database\n",
    "   db = client['test_database']\n",
    "   # Initiate Table\n",
    "   tab = db['test_table']\n",
    "   ```\n",
    "2. Insert an entry into the collection you have initiated. Check if the entry is inserted from the mongo shell.\n",
    "   Query the inserted entry from Pymongo.\n",
    "\n",
    "3. Try updating the entry you have inserted and verify that it has been updated.\n",
    "\n",
    "<br>\n",
    "\n",
    "##Part 2: Practice CSS Selectors\n",
    "\n",
    "CSS selectors are an important part of web scraping as they allow you to select content on a web page. Refer to \n",
    "this [document](css_selector_cheatsheet.pdf) if you need a reminder of the CSS selectors that are available to you.\n",
    "The best way to test out CSS selectors is to use the `Inspect Element` tool in the Google Chrome browser. We will\n",
    "walk through the work flow in the exercise below.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Open up `data/ebay_shoes.html` with Google Chrome. It should be as shown below.\n",
    "\n",
    "   ![image](pics/first_image.png)\n",
    "\n",
    "   <br>\n",
    "   \n",
    "2. Right click on the first shoe image and click `Inspect Element` to bring up a panel, as seen below.\n",
    "\n",
    "   ![image](pics/second_image.png)\n",
    "\n",
    "   <br>\n",
    "\n",
    "3. Construct a CSS select that would allow you to select every shoe image on the page.\n",
    "   The CSS selector should be referring to the `img` tag of a particular class. \n",
    "   Click `Console` to switch to the console mode. Select the CSS selector by entering into the console `$('your css selector')`. Hover over one of the selected elements to inspect it.\n",
    "\n",
    "   <br>\n",
    "\n",
    "4. Open up IPython in your terminal and import BeautifulSoup4 with the line `from bs4 import BeautifulSoup`.\n",
    "   Read `data/ebay_shoes.html` in as one string from the file and put it into a `BeautifulSoup()` with the \n",
    "   line `soup = BeautifulSoup(html_str, 'html.parser')`. \n",
    "\n",
    "   You should be able to use the CSS selector on the soup using `soup.select('your css selector')`. It will \n",
    "   then return a list of tags that each contains the source of the image location. Create a list of the paths to the    image locations by looping through the tags and accessing the image path by `tag['src']`.\n",
    "\n",
    "5. Open the file paths and read the files in as string. Write them to a new directory named `images`. Open the   \n",
    "   files in the `images` directory to ensure the images are saved properly.\n",
    "\n",
    "6. This is the basic work flow of web scraping with CSS selectors. Realistically you would not be reading in a local\n",
    "   html file, but read the html file from a link instead. Go to ebay, search for your a product of your choice and \n",
    "   copy the link. Use the `get` function in the [`requests` library](http://docs.python-requests.org/en/latest/)\n",
    "   to retrieve the html. The `get` function returns a response where `response.content` returns the html as a \n",
    "   string.\n",
    "   \n",
    "7. Retrieve the product descriptions from your selected page by following the web scraping work flow.  \n",
    "\n",
    "<br>\n",
    "\n",
    "##Part 4: Scraping using the meta data\n",
    "\n",
    "**At this point:**\n",
    "- **We have successful gathered article metadata from the NYT API**\n",
    "- **We have stored said data in MongoDB**\n",
    "- **We have URLs for each article that we can now use for scraping**\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Now that we have all the meta data, it it time to get the article content!  We will be doing something \n",
    "   I call a data join (some people call it [data blending](http://www.tableausoftware.com/videos/data-integration)... but they charge you money so they can call it that).\n",
    "    * Iterate over your collection in your database.  For all the articles for which you do not have HTML content \n",
    "      (this will be all of them to begin with), use the 'web url' in the meta data to make a web request.\n",
    "    \n",
    "  \n",
    "    * Use Beautiful Soup to parse the returned HTML. Make sure to initialize `soup` with:   \n",
    "      `soup = BeautifulSoup(response.text, 'html.parser')`\n",
    "      \n",
    "    * Add a new field in the meta data records in your Mongo database to store the raw HTML from the web page.\n",
    "    \n",
    "    * Find the CSS selectors that would allow you to extract article text in the web pages. You can use the Chrome \n",
    "      DevTools to help you find the relevant CSS Selectors. If you are having problems such as\n",
    "      `$ is not a function`. Use the following script to load in the jQuery library and then try again.\n",
    "  \n",
    "      ```js\n",
    "      var jq = document.createElement('script');\n",
    "      jq.src = \"https://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js\";\n",
    "      document.getElementsByTagName('head')[0].appendChild(jq);\n",
    "      ```\n",
    "      \n",
    "    * Use `soup.select(Your CSS Selector)` to extract article text from the web pages.\n",
    "    \n",
    "    * Add a new field in the meta data records in your Mongo database to store the text of the articles.\n",
    "   \n",
    "\n",
    "You have made it to the end (hopefully succcessfully).  Now that you have your data and have contextualized it with information from the web, you can start performing some interesting analyses on it.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Extra\n",
    "\n",
    "### User Login\n",
    "\n",
    "Scraping sites is easy when you can have clear access to the content and it is in a semi-structured form.  One issue that arises with scraping sites is the need to login.  \n",
    "\n",
    "1. Find a site that you might want to get data from that requires a login.\n",
    "\n",
    "* Use [Mechanize](http://wwwsearch.sourceforge.net/mechanize/) to login to a site to scrape.\n",
    "* [Mechanize For Beginners](http://www.pythonforbeginners.com/mechanize/browsing-in-python-with-mechanize/)\n",
    "* [Mechanize Login Tutorial](http://simplapi.wordpress.com/2012/04/20/pythons-mechanize-login-like-a-user/)\n",
    "* Example code in [advanced_scraping.ipynb](advanced_scraping.ipynb)\n",
    "\n",
    "### Client side templating\n",
    "\n",
    "Some sites do what is called [client side templating](http://www.smashingmagazine.com/2012/12/05/client-side-templating/), basically you send only data from the server and write a bunch of Javascript to dynamically write HTML when the page loads.\n",
    "\n",
    "1. Find a site that templates on the client.\n",
    "2. Look at the [Chrome Developer Tools](http://thewc.co/articles/view/web-inspector-tutorial) to find which \"Resource\" the data came in. __Hint Hint, look for a JSON file__\n",
    "\n",
    "### Automated Scraper\n",
    "\n",
    "Scraping the web from your laptop is great, but what if you want to automate things and have a long running process (scraping for days...)?  A common solution is to setup a script that continuous scrapes the web from a hosted virtual server on something like Amazon Web Service (AWS).\n",
    "\n",
    "1. Follow along on the auxiliary AWS sprint here: [https://github.com/zipfian/aws-and-the-cloud](https://github.com/zipfian/aws-and-the-cloud#assignment)\n",
    "2. Setup an automated scraper on AWS using [cron](http://www.unixgeeks.org/security/newbie/unix/cron-1.html) or a [python scheduler](https://github.com/dbader/schedule)\n",
    "3. Save the scraped files to [S3](http://aws.amazon.com/s3/) or a database.\n",
    "\n",
    "### Proxy\n",
    "\n",
    "Some sites block IPs from know bad actors.  LinkedIn actually does this for any Amazon Web Services (AWS) IP address.  But us being the smart data scientists we are, we will devise a solution!\n",
    "\n",
    "![](http://nickcavarretta.com.au/shitzu/uploads/2013/09/Scrape-all-the-things-300x225.png)\n",
    "\n",
    "[Tor](https://www.torproject.org/) is a lovely community built around anonymization on the web.  Tor allows for users to browse the web anonymously by daisy chaining a request across a network of nodes to hide the origin IP of the request.\n",
    "\n",
    "![tor](http://upload.wikimedia.org/wikipedia/commons/d/dc/Tor-onion-network.png)\n",
    "\n",
    "1. Using Tor and its [Python client](https://stem.torproject.org/tutorials.html), try to setup a scraper that is anonymized.  Bonus points if you host your scraper on AWS and successful can scrape LinkedIn profile pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will scrape reviews from Yelp.  Just like the NYT (and any large tech company really), Yelp's power comes from its data in aggregate.  They will happily give you information on one restaurant or the text of one review, but it is quite hard to get __ALL__ of the reviews for __ALL__ of the restaurants.\n",
    "\n",
    "We will be using the Yelp [API](http://www.yelp.com/developers/documentation) and the python [wrapper](https://github.com/Yelp/yelp-api/tree/master/v2/python).  Look at the examples for how to use the wrapper in Python.\n",
    "\n",
    "```python\n",
    "# example in python\n",
    "\n",
    "import yelp.search as yelp\n",
    "\n",
    "key = \"xxxx\"\n",
    "c_secret = \"xxxx\"\n",
    "token = \"xxxx\"\n",
    "t_secret = \"xxxx\"\n",
    "\n",
    "url_params = {'term' : 'bars'}\n",
    "\n",
    "yelp.request(params, key, c_secret, token, t_secret)\n",
    "```\n",
    "\n",
    "Before beginning, enter the following into iTerm (it's needed in search.py in the yelp folder):\n",
    "  ```python\n",
    "  pip install oauth2\n",
    "  ```\n",
    "\n",
    "## Part 1: Downloading\n",
    "\n",
    "Assume we are building a restaurant recommendation engine based on user reviews (or sentiment). The first step in this process is to get data.\n",
    "\n",
    "1. Just like with the NYT you need to register to get an API key [here](http://www.yelp.com/developers/manage_api_keys)\n",
    "1. Using the [Yelp API](http://www.yelp.com/developers/documentation/v2/search_api) find all Gastropubs in SF.\n",
    "2. Store these records into MongoDB.\n",
    "3. How many Gastropubs are there in SF?\n",
    "\n",
    "## Part 2: Parsing\n",
    "\n",
    "For the second part we will __PARSE__ reviews from Yelp.  Once we have the textual content of the reviews we can do interesting NLP on them such as classifying the cuisine of the restaurant, determining the sentiment of the review, or [aspect based summarization](http://www.ryanmcd.com/papers/local_service_summ.pdf) of reviews.\n",
    "\n",
    "> To avoid getting the Galvanize IP blocked by Yelp I have downloaded the relevant HTML pages in the `html` folder\n",
    "\n",
    "2. For each Restaurant count how many 5 star reviews it has gotten.\n",
    "  \n",
    "  Going one step further we will parse the textual content of each review.  \n",
    "\n",
    "3. Parse each review and create a dictionary mapping a username to the text of the review they gave.\n",
    "3. How would you get the metadata on __EVERY__ restaurant on Yelp in SF?  And get around their limits? (Don't do this because we may get our IP blocked 0_o)\n",
    "\n",
    "### Part 3: Crawling\n",
    "\n",
    "Again, to avoid getting blocked by Yelp we will crawl Wikipedia instead.  What distinguishes \"crawling\" from just downloading and parse web pages is typically the following of additional links.  For example we often want to:\n",
    "\n",
    "1. Download a page\n",
    "2. Find all the relevant links on that page\n",
    "3. Download each of the relevant links\n",
    "4. Rinse and Repeat\n",
    "\n",
    "--\n",
    "\n",
    "For this we will work with a safer web domain (Wikipedia) to get practice traversing links.\n",
    "\n",
    "1. Retrieve and store every article within 1 hop from the 'Zipf's law' article.\n",
    "  <b style=\"color: red\">Do not follow external links, only linked Wikipedia\n",
    "articles</b>\n",
    "\n",
    "  ___HINT: The Zipf's Law article should be located at: 'http://en.wikipedia.org/w\n",
    "/api.php?action=parse&format=json&page=Zipf's%20law'___\n",
    "\n",
    " We will get some practice now with regular expressions in order to search the content of the articles for the terms `Zipf` or `Zipfian`.  We only want articles that mention these terms in the displayed text however, so we must first remove all the unnecessary HTML tags and only keep what is in between the relevant tags.  Beautiful Soup makes this almost trivial.  Explore the documentation to find how to do this effortlessly: [http://www.crummy.com/software/BeautifulSoup/bs4/doc/](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "2. Find all articles that mention 'Zipf' or 'Zipfian' (case\n",
    "insensitive) in the text of the article.\n",
    " * Test out your Regular Expressions __before__ you run them over __every__document you have in your database: [http://pythex.org/](http://pythex.org/). Here is some useful documentation on regular expressions in Python: [http://docs .python.org/2/howto/regex.html](http://docs.python.org/2/howto/regex.html)\n",
    "\n",
    " ___HINT: There should be ~10 articles___\n",
    "\n",
    "3. Once you have identified the relevant articles, save them to a file for now, we do not need to persist them in the database.\n",
    "\n",
    "## Fin\n",
    "\n",
    "Congratulations!  You have successful acquired interesting text content from the Internet.  Now we can do all sorts of interesting analysis on this content or you can grab the text content of your favorite website :)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Wednesday\n",
    "---------------------\n",
    "- Watch **Information Retrieval**:\n",
    "    - [Introduction to Information Retrieval](https://class.coursera.org/nlp/lecture/178)\n",
    "    - [Term-Document Incidence Matrices](https://class.coursera.org/nlp/lecture/179)\n",
    "    - [The Inverted Index](https://class.coursera.org/nlp/lecture/180)\n",
    "    - [Query Processing with the Inverted Index](https://class.coursera.org/nlp/lecture/181)\n",
    "    - [Phrase Queries and Positional Indexes](https://class.coursera.org/nlp/lecture/182)\n",
    "- Supplemental reading: \n",
    "    - [MR+S Chapter 1: Boolean Retrieval](http://nlp.stanford.edu/IR-book/pdf/01bool.pdf)\n",
    "    - [MR+S Chapter 2: Term vocabulary and postings list, section 2.4](http://nlp.stanford.edu/IR-book/pdf/02voc.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
