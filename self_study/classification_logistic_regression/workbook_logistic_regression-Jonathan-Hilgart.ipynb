{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminative Classifier\n",
    "===\n",
    "\n",
    "![](images/discriminative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By The End of This Session You Will:\n",
    "---\n",
    "- Know the difference between discriminative and generative models\n",
    "- Be able to use the discriminative MaxEnt model to carry out NLP tasks\n",
    "\n",
    "__Supplemental to these videos:__\n",
    "- [video 1](https://class.coursera.org/nlp/lecture/38)\n",
    "- [video 2](https://class.coursera.org/nlp/lecture/51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminative vs Generative Models\n",
    "===\n",
    "\n",
    "So far we have covered an example of a generative model, Naive Bayes, for sentiment analysis and text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary \n",
    "---\n",
    "\n",
    "1. __Generative models compute joint probabilities__\n",
    "\n",
    "   - Joint probabilities of observing the data ___and___ each of the classes respectively,  $p(D, C)$\n",
    "   - Concretely, probability of observing a certain document and a positive sentiment class\n",
    "   - e.g. $p(\\text{this movie sucks}, \\text{positive sentiment})$ is low\n",
    "   - Revisiting Naive Bayes, \n",
    "   \n",
    "     $$p(D, C) = p(D | C) \\cdot p(C)$$\n",
    "     \n",
    "     $$\\text{Joint probability} = \\text{Likelihood} \\times \\text{Prior}$$\n",
    "     \n",
    "     <br>\n",
    "     \n",
    "   - Naive Bayes subsequently computes the posterior (conditional) probability\n",
    "   \n",
    "     $$p(C | D) = \\frac{p(D, C)}{p(D)}$$\n",
    "     \n",
    "     $$\\text{Posterior probability} = \\frac{\\text{Joint probability}}{\\text{Probability of observing data}}$$\n",
    "    \n",
    "   <br>\n",
    "\n",
    "2. __Discriminative compute conditional probabilities__\n",
    "\n",
    "   - Conditional probability of observing the data ___given___ each of the classes respectively, $p(C | D)$\n",
    "   - Concretely, probability of observing a certain document given a positive sentiment class\n",
    "   - e.g. $p(\\text{positive sentiment | this movie sucks})$ is low\n",
    "   - Revisting logistic regression,\n",
    "   \n",
    "     $$p(C|D) = \\frac{1}{1 + e^{\\beta0+ \\beta_1x_1 + \\beta_2x_2 ...}} $$\n",
    "     \n",
    "   <br>\n",
    "   \n",
    "3. __Training of discriminative models are more difficult than generative models__\n",
    "\n",
    "   - The parameters of a discriminative model, e.g $\\beta_1, \\beta_2...$, have to be opitimized via maximizing likelihood through gradient methods\n",
    "   - The parameters of a generative model are simply different frequencies of classes and features \n",
    "   - Generative models are quicker to train and easier to optimize\n",
    "   \n",
    "   <br>\n",
    "\n",
    "4. __Discriminative models are more accurate then generative models__\n",
    "\n",
    "   - As shown empirically by Klein and Manning (2002), discriminative models are superior in carrying of various NLP tasks\n",
    "   - Especially when discriminative models are used with smoothing / regularization\n",
    "   \n",
    "     <br>\n",
    "     \n",
    "     <img src=\"images/performance.png\" width=\"300px\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Check Questions\n",
    "---\n",
    "\n",
    "1) What is the fundamental difference between generative and discriminative models in terms of the probabilities that they are computing ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>\n",
    "Click here for solution to 1.\n",
    "</summary>\n",
    "`\n",
    "Discriminative models compute the conditional probability of observing a class given the data. \n",
    "\n",
    "Generative models compute the joint probability of observing a class and the data jointly.\n",
    "`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What are the practical trade-offs between using a discriminative and a generative model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>\n",
    "Click here for solution to 2.\n",
    "</summary>\n",
    "`\n",
    "Discriminative models are more diiffcult / take longer to train but are more accurate\n",
    "`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxEnt (Discriminative) Models\n",
    "===\n",
    "\n",
    "The MaxEnt model is able to account for prediction of multiple classes beyond a binary target (logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary \n",
    "---\n",
    "\n",
    "1. __MaxEnt is logistic regression with multiple target__\n",
    "\n",
    "   - Logistic regression only permits a 0/1 target, MaxEnt allows many\n",
    "   - MaxEnt is also known as softmax model\n",
    "   - Say I have features $x_1, x_2, x_3$ and classes $c_1, c_2, c_3$ and thre is only one correct class out of the 3 for each instance     \n",
    "   - __MaxEnt formula:__\n",
    "   \n",
    "     $$p(C_i|D) = \\frac{e^{\\beta_i^T x}}{\\sum_{j=1}^k e^{\\beta_j^T x}} $$\n",
    "     \n",
    "   <br>\n",
    "   \n",
    "2. __Pictorial representation of MaxEnt__\n",
    "\n",
    "   - The blue circles represent inputs (features) in the model\n",
    "   - $x_0$ is the bias term and $x_1$ and $x_2$ are the other features respectively \n",
    "   - The red, green and cyan ellipses are softmax functions applied to the dot product of the input and the weights ($\\beta$)\n",
    "   - Since there are three classes, the model is represented by 3 softmax functions (red, green and cyan)\n",
    "   - Each arrowed line represent a weight ($\\beta$). Since there are 3 features, each of the softmax models is represented by 3 weights / lines\n",
    "\n",
    "   ![](images/softmax.png)\n",
    "   \n",
    "   <br>\n",
    "\n",
    "3. __Sklearn implementation of MaxEnt__\n",
    "\n",
    "   - MaxEnt is implemented under logistic regression in sklearn. Learn more [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "   - The input to the MaxEnt model could be the bag of words representation or other user defined features\n",
    "   - The target would be sentiments or different classes of text\n",
    "   - Specifying regularization (`penalty` as `l1` or `l2`) is important for MaxEnt performance\n",
    "   - The hyperparameter `C` controls how much regularization is applied to the model\n",
    "   \n",
    "     ![](images/logistic.png)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
