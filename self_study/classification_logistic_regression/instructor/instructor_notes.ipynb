{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Logistic Regession\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "TODOs\n",
    "---\n",
    "\n",
    "- Why would you pick a generative model over discriminative, even though a generative model is more complex?\n",
    "\n",
    "\n",
    "- Indicator \n",
    "    - feature vector is a basis vector [0 1 0 0]\n",
    "        - f1 = word is \"football\" \n",
    "        - f2 = word is \n",
    "        - f3 = word is capitalized\n",
    "        - f\n",
    "        \n",
    "- Logistic regression models provide multi-category classification in cases where the categories are exhaustive and mutually exclusive. That is, every instance belongs to exactly one category.\n",
    "    - create dustbin category\n",
    "\n",
    "- The optimum parameters are the ones for which each feature's predicted expectation equals its empirical expectation. (In order to maximize the log likelihood of the maxent model, we take a derivative of logP(C|D,λ) with respect to λi. This gives us δlogP(C|D,λ) / δλi= actual count(fi, C) - predicted count (fi, λ).)\n",
    "\n",
    "- Convex optimization ftw!\n",
    "      \n",
    "Karush–Kuhn–Tucker (KKT) conditions (\n",
    " a solution in nonlinear programming to be optimal, provided that some regularity conditions are satisfied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Agenda\n",
    "---\n",
    "1. OYO\n",
    "1. Review Naive Bayes exercises\n",
    "1. Lecture\n",
    "1. Q & A\n",
    "1. RAT\n",
    "1. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OYO\n",
    "---\n",
    "\n",
    "__What is the difference between logistic regression and Naive Bayes?__\n",
    "\n",
    "Both are supervised classifers.\n",
    "\n",
    "Naive Bayes is joint probability model.\n",
    "Logistic regression is a condiational probability model.\n",
    "\n",
    "-----\n",
    "\n",
    "As others have said, they both train feature weights wj for the linear decision function ∑jwjxj (decide true if above 0, false if below).  The difference is how you fit the weights from training data.\n",
    "\n",
    "In NB, you set each feature's weight independently, based on how much it correlates with the label.  (Weights come out to be the features' log-likelihood ratios for the different classes.)\n",
    "\n",
    "In logistic regression, by contrast, you set all the weights together such that the linear decision function tends to be high for positive classes and low for negative classes.  (Linear SVM's work the same, except for a technical tweak of what \"tends to be high/low\" means.)\n",
    "\n",
    "The difference between NB and LogReg happens when features are correlated.  Say you have two features which are useful predictors -- they correlate with the labels -- but they themselves are repetitive, having extra correlation with each other as well.  NB will give both of them strong weights, so their influence is double-counted.  But logistic regression will compensate by weighting them lower.\n",
    "\n",
    "This is a way to view the probabilistic assumptions of the models; namely, Naive Bayes makes a conditional independence assumption, which is violated when you have correlated/repetitive features.\n",
    "\n",
    "One nice thing about NB is that training has no optimization step.  You just calculate a count table for each feature and you're done with it -- it's single pass and trivially parallelizable every which way.\n",
    "\n",
    "[Source](https://www.quora.com/What-is-the-difference-between-logistic-regression-and-Naive-Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAT Solutions\n",
    "-----\n",
    "\n",
    "1. Generative models seek to maximize joint likelihood.  \n",
    "Conditional models seek to maximize the conditional likelihood.\n",
    "\n",
    "2. Naive Bayes. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. \n",
    "\n",
    "3.  There are two features that match the movie Magic Mike and their weights are 0.9 and -.2. The other feature whose weight is 0.8 doesn't match the movie Magic Mike. Thus, 0.9 + (-.2) = .7\n",
    "\n",
    "4. One possible problem with Naive Bayes models is _________________.\n",
    "    A. mulitple features telling you the same thing\n",
    "    \n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "__DEPRECATED__:\n",
    "1. A (1) Instead of putting probabilities over observed data (i.e. $P(data,class)$ ) like joint models (e.g. Naive Bayes) do, conditional models take data as given and put probabilities over hidden structures (i.e. $P(class|data)$ ) because these probabilities are more closely related to classification success. Conditional models seek to maximize these conditional probabilities.\n",
    "\n",
    "2. A (1)  \n",
    "    \\[the curvaceous slopes\\] is not capitalized nor preceded by “of”  \n",
    "    \\[Stone Mountain\\] is not preceded by “of”\n",
    "3. .\n",
    "\n",
    "4. C (3) The probability that the preceding word occurs at the beginning of a sentence would be less useful compared to the other features because it doesn’t tell us much about whether a period ends a sentence or not. A similar feature that would be useful is the probability that the following word occurs at the beginning of a sentence.\n",
    "\n",
    "5. B (3) Adding overlapping features to the model does not affect the classification decision because they do not add more information. This means that it doesn't hurt the model to add overlapping features unlike a Naive Bayes classifer. The parameters of the features are adjusted as the overlapping features added. The parameters of the features are estimated so that empirical and predicted expectations of the feature are matched.\n",
    "\n",
    "6. False: In order to maximize the log likelihood of the maxent model, we take a derivative of logP(C|D,λ) with respect to λi. This gives us δlogP(C|D,λ) / δλi= actual count(fi, C) - predicted count (fi, λ). The optimum parameters are the ones that make this derivative equal to zero (i.e. predicted count is equal to its empirical count)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
