{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Logistic Regression: The workhorse of Data Science\n",
    "=====\n",
    "\n",
    "![](http://i.stack.imgur.com/bA57S.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "By the end of this session, you should be able to:\n",
    "---\n",
    "\n",
    "- Explain the difference between Logistic Regression and Naive Bayes\n",
    "- Explain the difference between Linear and Logistic Regression\n",
    "- Fit a multinomial Logistic Regression to text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Logistic Regression vs Naive Bayes\n",
    "---\n",
    "\n",
    "![](http://cdn2.hubspot.net/hubfs/426799/powertools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Generative models\n",
    "----   \n",
    "\n",
    "- P(c, d)\n",
    "- Seek to maximize the joint likelihood\n",
    "- Generate the observed data from hidden (latent) stuff, place probabilities over both observed data and the hidden stuff (e.g.: ngram, Naive Bayes)\n",
    "\n",
    "![](images/gen_model_formula.png)\n",
    "\n",
    "Create a joint model of the form p(y,x) = p(y)p(x|y) then condition on observed features x thereby deriving the class posterior p(y|x)\n",
    "\n",
    "![](images/gen_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "Discriminative models\n",
    "----\n",
    "\n",
    "- P(c|d)\n",
    "- Take the data as given, models __only__ the conditional probability of the class.\n",
    "- eg: Logistic regression and SVM\n",
    "\n",
    "![](images/disriminative_model_flow.png)\n",
    "\n",
    "![](images/disrim_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Generative Model vs Discriminative Model\n",
    "----\n",
    "\n",
    "A generative model learns the joint probability distribution p(x,y).\n",
    "\n",
    "A discriminative model learns the conditional probability distribution p(y|x).\n",
    "\n",
    "The data: (x,y): (1,0), (1,0), (2,0), (2, 1)\n",
    "\n",
    "| __p(x,y)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | 1/2 | 0 |\n",
    "| x=2 | 1/4 | 1/4 |\n",
    "\n",
    "\n",
    "| __p(y pipe x)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | 1 | 0 |\n",
    "| x=2 | 1/2 | 1/2 |\n",
    "\n",
    "The distribution p(y|x) is the natural distribution for classifying a given example x into a class y, which is why algorithms that model this directly are called discriminative algorithms. Generative algorithms model p(x,y), which can be tranformed into p(y|x) by applying Bayes rule and then used for classification. However, the distribution p(x,y) can also be used for other purposes. For example you could use p(x,y) to generate likely (x,y) pairs.\n",
    "\n",
    "[Source](http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Take home message\n",
    "----\n",
    "\n",
    "Generally, discriminative models outperform generative models in classification tasks.\n",
    "\n",
    "[On Discriminative vs. Generative\n",
    "classifiers: A comparison of logistic regression and naive Bayes ](http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "Is Naive Bayes a generative or discriminative model?\n",
    "</summary>\n",
    "Naive Bayes is a __generative__ model.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Regessions\n",
    "----\n",
    "\n",
    "![](http://www.marketingdistillery.com/wp-content/uploads/2014/11/TheThreeRegressions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.appstate.edu/~whiteheadjc/service/logit/logit.gif)\n",
    "\n",
    "[Read more here](images/http://faculty.cas.usf.edu/mbrannick/regression/Logistic.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "If I wanted to build a classifer to predict, click-through-rate (i.e., Will this person click on my ad and give me $$$), which type of regession would I use? \n",
    "</summary>\n",
    "Logistic Regression\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Logistic Regession: Linear Regresssion for Classification\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize linear regression to the (binary) classification setting by making two changes. \n",
    "\n",
    "1) Replace the Gaussian distribution for y with a Bernoulli distribution,which is more appropriate for the case when the response is binary, y ∈ {0, 1}. \n",
    "\n",
    "![](images/berm.png)\n",
    "\n",
    "2) Compute a linear combination of the inputs but then we pass this through a function that ensures 0 ≤ μ(x) ≤ 1 by defining:\n",
    "\n",
    "![](images/sigma.png)\n",
    "\n",
    "sigm(η) refers to the sigmoid function, also known as the logistic or logit function\n",
    "\n",
    "![](images/sigma_formula.png)\n",
    "\n",
    "Squashing function\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/2000px-Logistic-curve.svg.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Maps the whole real line to [0,1], which is necessary for the output to be interpreted as a probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put parts 1 and 2 together:\n",
    "\n",
    "![](images/logistic_regession.png)\n",
    "\n",
    "[Read more here](http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Logistic Regression: Beyond Binary\n",
    "----\n",
    "\n",
    "### \"Soft max\" \n",
    "\n",
    "Generalizes logistic regression to classification problems where the class label can take on more than 2 possible values.\n",
    "\n",
    "There is the baseline class (K) and compared to all other classes:\n",
    "\n",
    "![](images/multiclass.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "---\n",
    "Feature Engineering: Next to data cleaning, what you should spend your time doing\n",
    "----\n",
    "\n",
    "![](http://3.bp.blogspot.com/-v7RCgnSNLfA/U5stfq5zuJI/AAAAAAAADt4/9_87WaSa620/s1600/features-in-ML.jpg)\n",
    "\n",
    "- Feature engineering is Art & Science\n",
    "- Find candidate features, select, and weight them\n",
    "- The optimum parameters are the ones for which each feature's predicted expectation equals its empirical expectation.\n",
    "- Find candidates, select, and weighting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "Given that NLP models are sparse (it is not uncommon to have a more than 1 million parameters), what extra step do you need to do during Feature Engineering?\n",
    "</summary>\n",
    "Regularize  \n",
    "<br>\n",
    "![](http://i.stack.imgur.com/HwqTv.png)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Feature-Based Linear Classifiers\n",
    "----\n",
    "\n",
    "- Assign a weight λi to each feature $f_i$ then linear function to predict class\n",
    "- Make vote postive by taking expotential of linear weighting(=∑$λ_if_i$(c,d))\n",
    "- Normalize to transform into a probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Not Important (for this class)\n",
    "----\n",
    "\n",
    "- Bayes Net/Graphical Model/PGMs. Don't worry. We'll cover them in special topics\n",
    "- The notation. Machine learning notation is a mess. Use the notation from Jared's class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
