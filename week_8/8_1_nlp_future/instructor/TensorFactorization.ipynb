{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you ever wondered how Facebook does facial recognition? \n",
    "Or how to handle the data coming from thousands of IoT devices? \n",
    "\n",
    "In this lesson we'll cover the necessary mathematical techniques needed to work on these sorts of problems. To do so, we will need to generalize the concept of matrices to higher dimensions and resolve the implications this has on the tools, formulas, and theories we use in 2-D space. This requires building mathematical machinery to help us with these generalized matrices, called tensors, in order to be able to reconstruct them using a similar framework to singular value decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Recognition\n",
    "<img src = 'faces.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing data from sparse tensors in IoT \n",
    "<img src = 'IoT.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from scipy.io.matlab import loadmat\n",
    "import sktensor #https://github.com/mnick/scikit-tensor/tree/master/sktensor\n",
    "from sktensor import dtensor, cp_als, tucker\n",
    "import numpy as np\n",
    "from IPython.display import Latex, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement of Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a matrix $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}$ , there exists a factorization of $\\mathbf{A}$ of the form\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{A} &= \\mathbf{U\\Sigma V^T}\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\mathbf{U}$ is an $m\\times m$ unitary matrix,\n",
    "$\\mathbf{\\Sigma}$ is an $m\\times n$ diagonal matrix, and\n",
    "$\\mathbf{V}$ is an $n\\times n$ unitary matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "<img src = 'SVD.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that this means we can split the matrix $A$ into a **sum** of its parts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src ='SVDtrunc.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a basic example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mA = \n",
      "\u001b[0m\n",
      "[[1 0 0 0 2]\n",
      " [0 0 3 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 2 0 0 0]]\n",
      "\n",
      "\u001b[1;31mSVD = \n",
      "\u001b[0m\n",
      "[[ 1.  0.  0.  0.  2.]\n",
      " [ 0.  0.  3.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  2.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.matrix([[1,0,0,0,2],[0,0,3,0,0],[0,0,0,0,0],[0,2,0,0,0]])\n",
    "\n",
    "U,s,V = np.linalg.svd(A,full_matrices=False)\n",
    "\n",
    "S = np.diag(s)\n",
    "\n",
    "printred('A = \\n')\n",
    "print(A)\n",
    "print('')\n",
    "\n",
    "printred('SVD = \\n')\n",
    "print(np.dot(U,np.dot(S,V)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Notation and Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** A **Tensor** is a multidimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 3-D tensor would look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src = 'tensor.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defintion:** The **mode-$k$ unfolding** of a tensor $A \\in \\mathbb{R}^{I_1 \\times \\cdots \\times I_N}$ is a matrix $A_k \\in \\mathbb{R}^{I_k\\times \\Pi_{j\\neq k} I_j}$ \n",
    "\n",
    "<img src ='unfolding.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example that demonstrates the nuances of unfolding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mmode-1 matrix A1 =\n",
      "\u001b[0m\n",
      "[[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.]\n",
      " [ 13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.]]\n",
      "\n",
      "\u001b[1;31mmode-2 matrix A2 =\n",
      "\u001b[0m\n",
      "[[  1.   2.   3.   4.   5.   6.   7.   8.]\n",
      " [  9.  10.  11.  12.  13.  14.  15.  16.]\n",
      " [ 17.  18.  19.  20.  21.  22.  23.  24.]]\n",
      "\n",
      "\u001b[1;31mmode-3 matrix A3 =\n",
      "\u001b[0m\n",
      "[[  1.   2.   3.   4.   5.   6.]\n",
      " [  7.   8.   9.  10.  11.  12.]\n",
      " [ 13.  14.  15.  16.  17.  18.]\n",
      " [ 19.  20.  21.  22.  23.  24.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.linspace(1,24, 24)\n",
    "A = np.reshape(A, (2,3,4))\n",
    "\n",
    "## Skills Check! What size should these modes be?\n",
    "\n",
    "# Mode-1 matrix\n",
    "A1 = np.reshape(A, (A.shape[0], A.shape[1]*A.shape[2]))\n",
    "printred('mode-1 matrix A1 =\\n')\n",
    "print(A1)\n",
    "print('')\n",
    "\n",
    "# Mode-2 matrix\n",
    "A2 = np.reshape(A, (A.shape[1], A.shape[0]*A.shape[2]))\n",
    "printred('mode-2 matrix A2 =\\n')\n",
    "print(A2)\n",
    "print('')\n",
    "\n",
    "# Mode-3 matrix\n",
    "A3 = np.reshape(A, (A.shape[2], A.shape[0]*A.shape[1]))\n",
    "printred('mode-3 matrix A3 =\\n')\n",
    "print(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to work with tensors, we need to define certain operations on them, e.g., addition, multiplication. Fortunately, additional on tensors follows trivially from matrices, but there are some differences in how we define multiplication since we are in higher dimensional space. So how do we multiply tensors and matrices or tensors and vectors? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** The product of a tensor $X \\in \\mathbb{R}^{I_1 \\times\\cdots\\times I_N}$ times a matrix $U \\in \\mathbb{R}^{J\\times I_k}$ is denoted by $X\\times_k U$ and is of size $I_1\\times \\cdots \\times I_{k-1}\\times J\\times I_{k+1}\\times\\cdots\\times I_N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mY =\n",
      "\u001b[0m\n",
      "[[[  70.   76.]\n",
      "  [  82.   88.]\n",
      "  [  94.  100.]\n",
      "  [ 106.  112.]]\n",
      "\n",
      " [[ 151.  166.]\n",
      "  [ 181.  196.]\n",
      "  [ 211.  226.]\n",
      "  [ 241.  256.]]]\n",
      " \n",
      "\u001b[1;31mZ =\n",
      "\u001b[0m\n",
      "[[[  70.   76.]\n",
      "  [  82.   88.]\n",
      "  [  94.  100.]\n",
      "  [ 106.  112.]]\n",
      "\n",
      " [[ 151.  166.]\n",
      "  [ 181.  196.]\n",
      "  [ 211.  226.]\n",
      "  [ 241.  256.]]]\n",
      "\n",
      "Shape of tensor is (2, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE: Matrix multiplication of a (3,4,2) tensor with a (2,3) matrix along mode-1\n",
    "# Declare matrix and tensor\n",
    "\n",
    "## Skills Check! What should the size of the resulting tensor be?\n",
    "\n",
    "\n",
    "X = np.linspace(1,24,24)\n",
    "X = np.reshape(X,(3,4,2))\n",
    "U = np.linspace(1,6,6)\n",
    "U = np.reshape(U,(2,3))\n",
    "\n",
    "#create tensor object\n",
    "X = dtensor(X) \n",
    "\n",
    "# slice matrix by its frontal dimension\n",
    "X_11 = X[:,:,0]\n",
    "X_12 = X[:,:,1]\n",
    "\n",
    "# Tensor multiplication via numpy\n",
    "Y = np.zeros((2,4,2))\n",
    "Y[:,:,0] = np.dot(U,X_11)\n",
    "Y[:,:,1] = np.dot(U,X_12)\n",
    "printred('Y =\\n')\n",
    "print(Y)\n",
    "print(' ')\n",
    "\n",
    "# using the sktensor package\n",
    "Z = X.ttm(U,0)\n",
    "printred('Z =\\n')\n",
    "print(Z)\n",
    "\n",
    "# check shape\n",
    "print('')\n",
    "print('Shape of tensor is %s' % str(Z.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** The product of a tensor $X \\in \\mathbb{R}^{I_1 \\times\\cdots\\times I_N}$ times a vector $\\mathbf{v} \\in \\mathbb{R}^{I_k}$ is denoted by $X\\bullet_k \\mathbf{v}$ and is of size $I_1\\times \\cdots \\times I_{k-1}\\times I_{k+1}\\times\\cdots\\times I_N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mY=\n",
      "\u001b[0m\n",
      "[[  50.   60.]\n",
      " [ 130.  140.]\n",
      " [ 210.  220.]]\n",
      "\u001b[1;31m\n",
      "Z=\n",
      "\u001b[0m\n",
      "[[  50.   60.]\n",
      " [ 130.  140.]\n",
      " [ 210.  220.]]\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE: Vector multiplication of a (3,4,2) tensor with a vector of length 4\n",
    "# Declare vector and tensor\n",
    "X = np.linspace(1,24,24)\n",
    "X = np.reshape(X,(3,4,2))\n",
    "v = np.linspace(1,4, 4)\n",
    "\n",
    "# Create tensor object\n",
    "X = dtensor(X)\n",
    "\n",
    "# slice matrix by its frontal dimension\n",
    "X_11 = X[:,:,0]\n",
    "X_12 = X[:,:,1]\n",
    "\n",
    "# Tensor multiplication via numpy\n",
    "Y = np.zeros((3,2))\n",
    "Y[:,0] = np.dot(X_11,v)\n",
    "Y[:,1] = np.dot(X_12,v)\n",
    "printred('Y=\\n')\n",
    "print(Y)\n",
    "\n",
    "# using sktensor package\n",
    "Z = X.ttv(v,1)\n",
    "printred('\\nZ=\\n')\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor outer product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** The **vector outer product** of two vectors $\\mathbf{a}$ and $\\mathbf{b}$ is denoted by $\\mathbf{a}\\odot\\mathbf{b}$. Note that this result is a **matrix**. Hence, if $M = \\mathbf{a}\\odot\\mathbf{b}$ then the entries of $M$ are\n",
    "$$m_{ij} = a_i b_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Outer products should not be a new concept. This outer product is common in 2-D space, e.g., consider an $(m\\times 1)$ vector multiplied by a $(1\\times n)$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank of a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of a tensor causes difficulties when attempting to generalize matrix properties to higher order tensors. There are several possible generalizations of the notion of rank and we discuss some of them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** An $N$-way tensor, $X\\in \\mathbb{R}^{I_1\\times\\cdot\\times I_N}$, is **rank one** if it can be written as the outer product of $N$ vectors, i.e.,\n",
    "\n",
    "$$X = \\mathbf{a}^{(1)}\\odot \\mathbf{a}^{(2)} \\odot\\cdots\\odot \\mathbf{a}^{(N)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a rank one 3-way tensor looks like the figure below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='rankOneTensor.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** The **$n$-rank** of a tensor $X$, for $n = 1,\\ldots,N$, denoted by $\\operatorname{rank}_n(X)$, is the rank of the unfolding $X_n$.\n",
    "\n",
    "$$\\operatorname{rank}_n(X) := \\operatorname{rank}(X_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** The (outer-product) **rank of $X$**, denoted by $\\operatorname{rank}(X)$, is defined as the smallest integer $r$ such that $$X = \\sum_{i=1}^r a_1^i \\otimes a_2^i \\otimes \\cdots \\otimes a_N^i$$\n",
    "\n",
    "**In other words**, the rank of $X$ is the minimum number of rank-1 tensors that can sum up to $X$\n",
    "\n",
    "Visually, this looks like the figure below:\n",
    "<img src = 'parafac.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a higher order analogue  of SVD for tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would like to have a higher order analogue of SVD, but for tensors. Their roles would be similar to that of SVD and would allow similar numerical approximations, lower-rank SVDs that reduce memory consumption, allow for principal component analysis, and have increased computational speed. Since the definition of rank is much more complex for tensors than for matrices, this leads us to some interesting results about tensor SVDs.\n",
    "\n",
    "\n",
    "**Question:** Under what condition on the rank does a tensor allow a diagonal decomposition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences between SVD and HO-SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the names might be similar, there are many differences between SVD for matrices and HO-SVD for tensors.\n",
    "\n",
    "* The core tensor in HO-SVD is not necessarily diagonal like $\\Sigma$ is in SVD; it may even be dense\n",
    "* There is no finite algorithm for computing the rank of a tensor\n",
    "* No algorithm for computing the upper bound of the rank of a tensor\n",
    "    * 100% of $2\\times 2$ matrices have rank 2\n",
    "    * 79% of $2\\times 2\\times 2$ tensors have rank 2\n",
    "    * 21% of $2\\times 2\\times 2$ tensors have rank 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the core tensor is diagonal, then the tensor SVD looks like this:\n",
    "<img src = 'HOSVD.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tucker Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If the core tensor in SVD is **not** diagonal, then we can calculate a different type of SVD, called a Tucker Decomposition:\n",
    "<img src = 'tucker.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tucker decomposition (HO-SVD) algorithm can be built from several SVDs as follows:\n",
    "\n",
    "1. Given a tensor $A \\in \\mathbb{R}^{I_1\\times \\cdots \\times I_N}$ construct the mode-$k$ unfolding matrix $A_k$ \n",
    "\n",
    "2. Compute the SVD of $A_k = U_k \\Sigma_k V_k^T$ and store the side matrices $U_k$\n",
    "\n",
    "3. The core tensor $S$ is then the projection of $A$ onto the tensor basis formed by factor matrices $\\{U_k\\}_{n=1}^N$, i.e., $S = A \\times_{n=1}^N U_k^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Tucker Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Core $\\mathcal{G}$ may be dense\n",
    "* $A,B,C$ are generally orthonormal\n",
    "* Not unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Uniqueness of Tucker Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Declaration of tensor\n",
    "T = np.zeros((3, 4, 2))\n",
    "T[:, :, 0] = [[ 1,  4,  7, 10], [ 2,  5,  8, 11], [3,  6,  9, 12]]\n",
    "T[:, :, 1] = [[13, 16, 19, 22], [14, 17, 20, 23], [15, 18, 21, 24]]\n",
    "T = dtensor(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def myhosvd(T):\n",
    "\n",
    "    T1 = np.reshape(T,(np.shape(T)[0],np.shape(T)[1]*np.shape(T)[2]))\n",
    "    U1,S1,V1 = np.linalg.svd(T1)\n",
    "\n",
    "    T2 = np.reshape(T, (np.shape(T)[1], np.shape(T)[0]*np.shape(T)[2]))\n",
    "    U2,S2,V2 = np.linalg.svd(T2)\n",
    "\n",
    "    T3 = np.reshape(T, (np.shape(T)[2], np.shape(T)[0]*np.shape(T)[1]))\n",
    "    U3,S3,V3 = np.linalg.svd(T3)\n",
    "    \n",
    "    S = T.ttm(U1.T, 0).ttm(U2.T, 1).ttm(U3.T, 2)\n",
    "    \n",
    "    return S, U1, U2, U3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = tucker.hooi(T, [3, 4, 2], init='nvecs')\n",
    "S, U1, U2, U3 = myhosvd(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mY = \n",
      "\u001b[0m\n",
      "(dtensor([[[  6.96306024e+01,  -1.13159238e-02],\n",
      "        [ -1.81302157e-02,  -6.91903815e+00],\n",
      "        [ -8.85917216e-15,  -5.36191225e-14],\n",
      "        [  2.02550947e-14,  -6.70103577e-15]],\n",
      "\n",
      "       [[ -7.01201417e-02,  -1.61083107e+00],\n",
      "        [ -7.83965156e-01,  -7.00974052e-01],\n",
      "        [ -6.31538362e-15,  -5.56604952e-15],\n",
      "        [ -1.74939015e-15,  -7.04039172e-16]],\n",
      "\n",
      "       [[ -6.31266418e-15,   5.74500373e-14],\n",
      "        [  2.88718602e-14,   2.43932168e-14],\n",
      "        [ -1.35683120e-15,   5.12482366e-16],\n",
      "        [ -9.30383510e-16,  -1.68608668e-16]]]), [array([[ 0.54117833,  0.7351594 , -0.40824829],\n",
      "       [ 0.57662442,  0.02894161,  0.81649658],\n",
      "       [ 0.6120705 , -0.67727619, -0.40824829]]), array([[ 0.34444166,  0.76246964, -0.        , -0.54772256],\n",
      "       [ 0.44038579,  0.32566908, -0.40824829,  0.73029674],\n",
      "       [ 0.53632993, -0.11113147,  0.81649658,  0.18257419],\n",
      "       [ 0.63227407, -0.54793202, -0.40824829, -0.36514837]]), array([[ 0.35334125,  0.9354945 ],\n",
      "       [ 0.9354945 , -0.35334125]])])\n",
      "\n",
      "\u001b[1;31mmy HOSVD =\n",
      "\u001b[0m\n",
      "[[[ -6.57278930e+01   2.20172049e+01]\n",
      "  [ -2.87994061e-01  -1.74724158e-02]\n",
      "  [  3.13422512e+00   1.71109631e+00]\n",
      "  [ -7.82073737e+00  -4.16925911e+00]]\n",
      "\n",
      " [[ -6.52367917e-01  -1.54415629e+00]\n",
      "  [  1.24967437e-02   6.59378684e-04]\n",
      "  [ -3.40966307e-01  -1.06290486e-01]\n",
      "  [  8.37274513e-01   2.58681527e-01]]\n",
      "\n",
      " [[ -1.18400945e-14  -3.23650732e-16]\n",
      "  [  5.98521421e-16   7.25279320e-17]\n",
      "  [ -5.89353809e-16  -4.93491696e-17]\n",
      "  [ -4.51928149e-16   2.86871474e-16]]] [[-0.54117833  0.7351594   0.40824829]\n",
      " [-0.57662442  0.02894161 -0.81649658]\n",
      " [-0.6120705  -0.67727619  0.40824829]] [[-0.41659937  0.50846418 -0.06016458  0.75119197]\n",
      " [-0.46768695 -0.49138045 -0.73457938  0.01439793]\n",
      " [-0.52258651 -0.50895688  0.67530401  0.10876915]\n",
      " [-0.57846108  0.49088932  0.02716337 -0.65090138]] [[-0.62468669 -0.7808755 ]\n",
      " [-0.7808755   0.62468669]]\n"
     ]
    }
   ],
   "source": [
    "printred('Y = \\n')\n",
    "print(Y)\n",
    "print('')\n",
    "printred('my HOSVD =\\n')\n",
    "print(S, U1, U2, U3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the above two solutions are different, but if they can reconstruct the original tensor, $T$, then we have proven that Tucker Decomposition is not unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mmy HOSVD = \n",
      "\u001b[0m\n",
      "[[[  1.  13.]\n",
      "  [  4.  16.]\n",
      "  [  7.  19.]\n",
      "  [ 10.  22.]]\n",
      "\n",
      " [[  2.  14.]\n",
      "  [  5.  17.]\n",
      "  [  8.  20.]\n",
      "  [ 11.  23.]]\n",
      "\n",
      " [[  3.  15.]\n",
      "  [  6.  18.]\n",
      "  [  9.  21.]\n",
      "  [ 12.  24.]]]\n",
      "\n",
      "\u001b[1;31mSKTENSOR = \n",
      "\u001b[0m\n",
      "[[[  1.  13.]\n",
      "  [  4.  16.]\n",
      "  [  7.  19.]\n",
      "  [ 10.  22.]]\n",
      "\n",
      " [[  2.  14.]\n",
      "  [  5.  17.]\n",
      "  [  8.  20.]\n",
      "  [ 11.  23.]]\n",
      "\n",
      " [[  3.  15.]\n",
      "  [  6.  18.]\n",
      "  [  9.  21.]\n",
      "  [ 12.  24.]]]\n"
     ]
    }
   ],
   "source": [
    "printred('my HOSVD = \\n')\n",
    "print(S.ttm(U1,0).ttm(U2,1).ttm(U3,2))\n",
    "print('')\n",
    "printred('SKTENSOR = \\n')\n",
    "print(Y[0].ttm(Y[1][0], 0).ttm(Y[1][1], 1).ttm(Y[1][2], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSize of A = \n",
      "\u001b[0m\n",
      "216128\n",
      "\n",
      "\u001b[1;31mSize of Z = \n",
      "\u001b[0m\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "d = 30\n",
    "A = np.zeros((d,d,d))\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        for k in range(A.shape[2]):\n",
    "            A[i,j,k] = 1./np.sqrt((i+1)**2 + (j+1)**2 + (k+1)**2)\n",
    "            \n",
    "            \n",
    "printred('Size of A = \\n')\n",
    "print(sys.getsizeof(A))\n",
    "print('')\n",
    "\n",
    "A = dtensor(A)\n",
    "Z = tucker.hooi(A, [d,d,d], init='nvecs')\n",
    "printred('Size of Z = \\n')\n",
    "print(sys.getsizeof(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Efficiency\n",
    "\n",
    "Below we check for the computational speed of the two algorithms and compare them for small tensors and large tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 18.2 ms per loop\n",
      "1 loop, best of 3: 183 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit tucker.hooi(A, [d,d,d], init='nvecs')\n",
    "%timeit myhosvd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 6.2 ms per loop\n",
      "1000 loops, best of 3: 411 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit tucker.hooi(T, [3,4,2], init='nvecs')\n",
    "%timeit myhosvd(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lecture, we generalized the notion of matrices to higher dimensional spaces. This generalization increased the complexity of rank, SVD, matrix multiplication, and vector multiplication. Furthermore, we explored the areas in which the diagonal SVD exists and when it has to be approximated by a Tucker Decomposition. The computational demands can be decreased by more efficient algorithms and or lower-rank decompositions.\n",
    "\n",
    "And now we know one very crucial aspect of how facebook is able to do facial recognition so quickly. By decomposing higher dimensional matrices (facial data from images) into approximate lower rank cores (using HO-SVD), computational demands can remain practical and their able to tell you your name from just your face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from IPython.core.display import HTML\n",
    "#def css_styling():\n",
    "#    styles = open(\"custom.css\", \"r\").read()\n",
    "#    return HTML(styles)\n",
    "#css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printred(string):\n",
    "    print('\\x1b[1;31m'+'%s'%string +'\\x1b[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Diagrams came from: http://www.cs.cmu.edu/~christos/TALKS/SIGMOD-07-tutorial/tensor3.pdf\n",
    "\n",
    "Material came from the following sources:\n",
    "\n",
    "ftp://ftp.esat.kuleuven.be/pub/SISTA/ida/reports/97-75.pdf\n",
    "\n",
    "http://www.sandia.gov/~tgkolda/pubs/pubfiles/SAND2007-6702.pdf\n",
    "\n",
    "http://www.chemometrics.ru/materials/presentations/wsc6/T09.pdf\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tensor_rank_decomposition\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
